\section{Architecture}
\label{sec:architecture}


\begin{table*}[t]
	\centering
	\begin{tabular}{ll|ll|ll|ll}
		\hline
		\multicolumn{2}{c|}{Initialization} &
		\multicolumn{2}{c|}{Connection Mgmt} &
		\multicolumn{2}{c|}{Data Transmission} &
		\multicolumn{2}{c}{Process Mgmt} \\
		\hline
		API & Cat. &
		API & Cat. &
		API & Cat. &
		API & Cat. \\
		\hline
		\hline
		\textbf{socket} & Local &
		\textbf{connect} & NoPart &
		\textbf{send(to,(m)msg)} & P2P-NB &
		\textit{(v)fork} & NoPart \\
		\hline
		bind & NoPart &
		\textbf{accept(4)} & P2P-B &
		\textbf{recv(from,(m)msg)} & P2P-NB &
		\textit{pthread\_create} & NoPart \\
		\hline
		listen & Local &
		\textbf{\textit{fcntl, ioctl}} & Local &
		\textbf{\textit{write(v)}} & P2P-NB &
		\textit{clone} & NoPart \\
		\hline
		socketpair & Local &
		\textbf{(get,set)sockopt} & Local &
		\textbf{\textit{read(v)}} & P2P-NB &
		\textit{execve} & NoPart \\
		\hline
		getsockname  & Local &
		\textbf{\textit{close}, shutdown} & P2P-NB &
		\textbf{\textit{memcpy}} & Local &
		\textit{exit} & P2P-NB \\
		\hline
		\textbf{\textit{malloc}} & Local &
		getpeername & Local &
		\textit{(p)select} & $\approx$Local &
		\textit{sleep} & P2P-NB \\
		\hline
		\textbf{\textit{realloc}} & Local &
		\textit{dup(2)} & P2P-NB &
		\textit{(p)poll} & $\approx$Local &
		\textit{daemon} & P2P-NB \\
		\hline
		\textit{epoll\_create} & Local &
		\textbf{\textit{epoll\_ctl}} & Local &
		\textbf{\textit{epoll\_(p)wait}} & $\approx$Local &
		\textit{sigaction} & Local \\
		\hline
	\end{tabular}
	\caption{Linux APIs that are related to socket and intercepted by \libipc{}. Categories include local, mostly local ($\approx$Local), peer-to-peer non-blocking (P2P-NB), peer-to-peer blocking wait (P2P-B) and non-partitionable (NoPart). APIs in \textit{italic} indicate usages more than socket. APIs in \textbf{bold} are called more frequently than others.}
	\label{tab:socket-api}
\end{table*}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Architecture of \sys{}.}
	\label{fig:architecture}
\end{figure}


To remove the overhead of context switch and centralized coordination while preserving the semantics and isolation of Linux system calls, we rethink the OS architecture and propose four design principles:

\textbf{Performance-critical OS functions run in user-mode library.}
Inspired by Library Operating System~\RED{(cite)} and user-mode network stacks~\RED{(cite)}, we move most OS functions from the kernel to user mode, \textit{i.e.}, file system and network sockets. We leverage multiple queues in contemporary NICs and NVMe SSDs to enable user-space direct access to network and storage. The kernel is still responsible for process creation, scheduling, virtual memory and device management, but no longer on the critical path of performance.

To maintain compatibility with existing Linux applications, we design a user-mode library \libipc as a drop-in replacement of the system call wrappers in the GNU C library (glibc). \libipc implements file system and network socket functions in user mode, and adds a wrapper to other system calls to track process creation and memory allocation. Figure~\ref{fig:architecture} shows the architecture of \sys{}.

\textbf{Processes as a peer-to-peer message passing system.}
For multi-core and multi-server scalability, we use message passing for inter-process communication and minimize shared memory writes, thus avoiding synchronization. In Table~\ref{tab:socket-api}, we categorize Linux socket operations into local, peer-to-peer and non-partitionable ones. We say an operation is \textit{local} if it only changes states local to a process (\textit{e.g.} assigning file descriptors). An operation is \textit{peer-to-peer} if the states are written by one processes and read by another process (\textit{e.g.} sending a message). We develop a high performance lockless shared memory queue to enable efficient peer-to-peer communication (Sec.\ref{subsec:lockless-queue}). Some peer-to-peer operations require blocking to wait for response from the peer, while others can return after a non-blocking write to the inter-process queue. We design to minimize blocking wait. Local and peer-to-peer operations are naturally scalable because their states are partitionable~\cite{partitionable}. For \textit{non-partitionable} operations (\textit{e.g.} load balancing new connections to listening processes), \libipc revisits the idea of \textit{monitor process}~\cite{hoare1974monitors} and delegates coordination to a monitor process. Delegation not only has higher throughput than synchronization~\cite{roghanchi2017ffwd}, but also ensures security and fairness.

%We develop a high performance lockless shared memory queue  to enable two application processes or one application process and the monitor process to communicate with each other directly. The Linux kernel provides memory isolation among different shared-memory queues. To reduce memory footprint and polling overhead, all socket connections between two processes are multiplexed through one lockless queue. In most cases, event polling and notifications are done locally (Sec.\ref{subsec:epoll}). As a result, only process initialization and connection setup are coordinated by the monitor.

\textbf{Design for the common and prepare for the worst.}
As stated in Sec.\ref{subsec:challenges}, sockets can be shared among threads and processes. To ensure correctness for concurrent access, mutual exclusion would sacrifice the common-case one-to-one communication performance. To avoid such synchronization overhead, \libipc creates a queue between each pair of sender and receiver processes, and ensures receive ordering and liveness in all scenarios (Sec.\ref{subsec:fork}).

We use cooperative multitasking and polling to reduce inter-core interrupts, signaling and scheduling costs in the kernel. When all processes are busy and responsive to incoming messages, the system would achieve high performance. However, we should ensure worst-case liveness of the system. To reduce polling overhead, \libipc puts the process to sleep after being idle for some time. \libipc uses Linux signals to interrupt long-running application code and wake up sleeping or blocked processes. When a process with \libipc is dead, the monitor detects and garbage collects it (Sec.\ref{subsec:process-mux}).

\textbf{Utilize hardware for performance.}
To achieve zero-copy for sending and receiving large buffers inside a same server, we utilize the virtual memory of CPU and modify page mapping on \texttt{memcpy}, \texttt{send} and \texttt{recv} (Sec.\ref{subsec:zerocopy}). For inter-server communication between RDMA capable servers, \libipc{} uses one-sided RDMA~\cite{mitchell2013using,kaminsky2016design} to offload the network stack to NIC hardware (Sec.\ref{subsec:rdma}). To communicate with standard TCP/IP endpoints, we use a light-weight user-space TCP/IP stack~\cite{dunkels2001design}, while leverage connection multiplexing and scatter-gather of the NIC (Sec.\ref{subsec:lwip}). In all three cases above, \libipc{} achieves zero copy and kernel bypass. %For storage, \libipc utilizes SPDK~\cite{spdk} to create a hardware queue for each process (Sec.\ref{sec:implementation}).


%\RED{Why do we take the idea of Library OS? Every design choice should have rationale, otherwise the paper would look like a technical report.}
%\sys takes the idea of Library Operating System \RED{(cite)}. All the threads are treated as separated processes in our design. All function calls to the GNU C library is redirected to our user-mode library \libipc.

%\libipc leverages message passing to communicate with other processes to conduct inter-process communication and coordination of the resources of the operating system, i.e., file system, networking (sockets) in user-mode. We take advantage of the unmodified Linux Kernel for process creation and isolation, scheduling and memory management.

%To make a high performance design of \libipc, we treat processes as a distributed system and separate the Linux API to scalable and non-scalable parts. For the scalable parts eg. assign file descriptor, the \libipc of each process settle them individually. For the non-scalable parts, \libipc revisit the idea of a single monitor process on each machine to tackle the coordination problem without the overhead of traditional distributed systems.

%We noticed that the read and write between two processes can be highly scalable if two process communicate with each other directly with a shared memory queue. As a result, only the connection setup is coordinated by the monitor, which alleviate the workload of the monitor. The permission of Linux Kernel could provide isolation for different connections. RDMA is used in our design for inter-process communication across multiple machines.
