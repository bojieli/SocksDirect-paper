\section{Architecture}
\label{sec:architecture}


\begin{table*}[t]
	\centering
	\begin{tabular}{ll|ll|ll|ll}
		\hline
		\multicolumn{2}{c|}{Initialization} &
		\multicolumn{2}{c|}{Connection Mgmt} &
		\multicolumn{2}{c|}{Data Transmission} &
		\multicolumn{2}{c}{Process Mgmt} \\
		\hline
		API & Cat. &
		API & Cat. &
		API & Cat. &
		API & Cat. \\
		\hline
		\hline
		\textbf{socket} & Local &
		\textbf{connect} & NoPart &
		\textbf{send(to,(m)msg)} & P2P &
		\textit{(v)fork} & NoPart \\
		\hline
		bind & NoPart &
		\textbf{accept(4)} & P2P &
		\textbf{recv(from,(m)msg)} & P2P &
		\textit{pthread\_create} & NoPart \\
		\hline
		listen & NoPart &
		\textbf{\textit{fcntl, ioctl}} & Local &
		\textbf{\textit{write(v)}} & P2P &
		\textit{clone} & NoPart \\
		\hline
		socketpair & Local &
		\textbf{(get,set)sockopt} & Local &
		\textbf{\textit{read(v)}} & P2P &
		\textit{execve} & NoPart \\
		\hline
		getsockname  & Local &
		\textbf{\textit{close}, shutdown} & P2P &
		\textbf{\textit{memcpy}} & Local &
		\textit{exit} & P2P \\
		\hline
		\textbf{\textit{malloc}} & Local &
		getpeername & Local &
		\textit{(p)select} & P2P &
		\textit{sleep} & P2P \\
		\hline
		\textbf{\textit{realloc}} & Local &
		\textit{dup(2)} & P2P &
		\textit{(p)poll} & P2P &
		\textit{daemon} & P2P \\
		\hline
		\textit{epoll\_create} & Local &
		\textbf{\textit{epoll\_ctl}} & Local &
		\textbf{\textit{epoll\_(p)wait}} & P2P &
		\textit{sigaction} & Local \\
		\hline
	\end{tabular}
	\caption{Linux APIs that are related to socket and intercepted by \libipc{}. Categories include local, peer-to-peer (P2P) and non-partitionable (NoPart). APIs in \textit{italic} indicate usages more than socket. APIs in \textbf{bold} are called more frequently than others.}
	\label{tab:socket-api}
\end{table*}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Architecture of \sys{}.}
	\label{fig:architecture}
\end{figure}


To address the performance and compatibility challenges, we propose the following design principles:

\textbf{User-mode socket library as a drop-in replacement.}
Inspired by Unikernels~\cite{madhavapeddy2013unikernels}, we move networking and IPC functions from the kernel to user space. Similar to existing works~\cite{peter2016arrakis,jeong2014mtcp,libvma}, we leverage multiple queues in modern NICs to enable user-space direct access to network. %The kernel is still responsible for process creation, scheduling, virtual memory and device management, but no longer on the critical path of performance.
To maintain compatibility with existing Linux applications, we design a user-mode library \libipc as a drop-in replacement of the system call wrappers in the GNU C library (glibc). \libipc{} implements network socket functions in user mode, and adds a wrapper to other system calls to track process creation and memory allocation. \libipc{} is not considered a secure domain, as it shares memory space with the application. Figure~\ref{fig:architecture} shows the architecture of \sys{}.

\textbf{Consider processes as a distributed system.}
For multi-core and multi-server scalability, we take the Multikernel~\cite{baumann2009multikernel} philosophy and use message passing for inter-process communication to avoid state sharing and synchronization. We consider processes in a server as a distributed system, in the sense that a) processes share nothing but peer-to-peer communication channels, b) new processes may join at any time, c) a process may crash unexpectedly and d) no process is trusted in terms of security. Instead of relying security on the isolation between application and ``protected kernel code'', we design a protocol among processes and delegate privileged operations to a special \textit{monitor} process, analogous to the coordinator in a distributed system.

To design the inter-process protocol with minimal message passing, in Table~\ref{tab:socket-api}, we categorize Linux socket operations into local, peer-to-peer and non-partitionable ones. We say an operation is \textit{local} if it only changes states local to a process (\textit{e.g.} assigning file descriptors). An operation is \textit{peer-to-peer} if the states are written by one processes and read by another process (\textit{e.g.} sending a message). We develop a high performance lockless queue to enable efficient peer-to-peer communication using shared memory and one-sided RDMA (Sec.\ref{subsec:lockless-queue}). We design all peer-to-peer operations to be non-blocking, \textit{i.e.} returns immediately after writing to the queue. Local and peer-to-peer operations are naturally scalable because their states are partitionable~\cite{partitionable}. For \textit{non-partitionable} and privileged operations (\textit{e.g.} \texttt{fork} and shared memory allocation), \libipc revisits the idea of \textit{monitor process}~\cite{hoare1974monitors} and delegates coordination to a monitor process. Delegation not only has higher throughput than synchronization~\cite{roghanchi2017ffwd}, but also ensures security and fairness.

\textbf{Each memory location is written by only one process.}
We design a high performance lockless queue to enable two application processes or one application process and the monitor process to communicate efficiently. To minimize cache migration in shared memory, make use of one-sided RDMA and ensure security, we design each memory location in the queue to be written by only one process. The kernel provides memory isolation among different queues. To reduce memory footprint and polling overhead, all socket connections between two processes are multiplexed through one queue.

\textbf{Optimize for the common and prepare for the worst.}
Sockets can be shared among threads and processes. To ensure correctness for concurrent access, mutual exclusion would sacrifice the common-case one-to-one communication performance. To avoid such synchronization overhead, \libipc creates a queue between each pair of sender and receiver processes, and ensures receive ordering and liveness in all scenarios (Sec.\ref{subsec:fork}).

We use cooperative multitasking and polling to reduce inter-core interrupts, signaling and scheduling costs in the kernel. When all processes are busy and responsive to incoming messages, the system would achieve high performance. However, we should ensure worst-case liveness of the system. To reduce polling overhead, \libipc puts the process to sleep after being idle for some time. \libipc uses Linux signals to interrupt long-running application code and wake up sleeping or blocked processes. When a process with \libipc is dead, the monitor detects and garbage collects it (Sec.\ref{subsec:process-mux}).

\textbf{Utilize hardware for performance.}
To achieve zero-copy for sending and receiving large buffers inside a same server, we utilize the virtual memory of CPU and modify page mapping on \texttt{memcpy}, \texttt{send} and \texttt{recv} (Sec.\ref{subsec:zerocopy}). For inter-server communication between RDMA capable servers, \libipc{} uses one-sided RDMA~\cite{mitchell2013using,kaminsky2016design} to offload the network stack to NIC hardware. To communicate with standard TCP/IP endpoints, we use a light-weight user-space TCP/IP stack~\cite{dunkels2001design}, while leverage connection multiplexing and scatter-gather of the NIC. In all three cases above, \libipc{} achieves zero copy and kernel bypass.


%\RED{Why do we take the idea of Library OS? Every design choice should have rationale, otherwise the paper would look like a technical report.}
%\sys takes the idea of Library Operating System \RED{(cite)}. All the threads are treated as separated processes in our design. All function calls to the GNU C library is redirected to our user-mode library \libipc.

%\libipc leverages message passing to communicate with other processes to conduct inter-process communication and coordination of the resources of the operating system, i.e., file system, networking (sockets) in user-mode. We take advantage of the unmodified Linux Kernel for process creation and isolation, scheduling and memory management.

%To make a high performance design of \libipc, we treat processes as a distributed system and separate the Linux API to scalable and non-scalable parts. For the scalable parts eg. assign file descriptor, the \libipc of each process settle them individually. For the non-scalable parts, \libipc revisit the idea of a single monitor process on each machine to tackle the coordination problem without the overhead of traditional distributed systems.

%We noticed that the read and write between two processes can be highly scalable if two process communicate with each other directly with a shared memory queue. As a result, only the connection setup is coordinated by the monitor, which alleviate the workload of the monitor. The permission of Linux Kernel could provide isolation for different connections. RDMA is used in our design for inter-process communication across multiple machines.
