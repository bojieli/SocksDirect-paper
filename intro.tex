\section{Introduction}
\label{sec:intro}

Most cloud applications use the socket API for inter-process communication among components or containers inside a same server and across data center network, in addition to serving Internet users. For example, communication intensive applications (\textit{e.g.} nginx and memcached) spend 50\%$\sim$90\% of CPU time in the OS kernel, mostly processing socket operations.
The overhead of Linux socket attributes to kernel crossing in system calls, context switch, process scheduling, synchronization, memory copy, cache miss and TCP transport.
Applications in high performance computing have a long tradition of using shared memory for intra-server communication and RDMA for inter-server, thus avoiding the overheads above. However, these abstractions are radically different from socket, and it is painful to port socket applications to shared memory and RDMA.
%The overhead of Linux socket becomes salient given the rapid growth of network speed and number of CPU cores per server. %We benchmark communication intensive applications (\textit{e.g.} Nginx and memcached) and find that 50\%$\sim$90\% of CPU time is spent in the OS kernel. When more CPU cores are utilized, they even spend a larger portion of time in the kernel~\cite{boyd2010analysis}. In addition to CPU overhead, latency is also a problem. The round-trip time (RTT) between two processes communicating with shared memory can be as low as 0.2$\mu$s, while TCP socket RTT between two cores is $\approx$16$\mu$s. In a data center, the RTT between RDMA servers is also one order of magnitude lower than kernel TCP/IP stack.

%There has been extensive work aiming to release the bare metal performance of multi-core CPU and data center network. For intra-server communication, there are mainly three lines of research. The first category of work use the NIC as a switch~\cite{peter2016arrakis,belay2017ix,yasukata2016stackmap}, but going deep to the NIC introduces $\approx2 \mu$s delay due to PCIe latency, one order of magnitude higher than shared memory. A second line of work optimize or redesign the kernel socket stack~\cite{lin2016scalable,han2012megapipe,jeong2014mtcp,baumann2009multikernel}, where the kernel uses peer-to-peer shared memory communication among cores. However, this approach does not eliminate context switch overhead, while system call batching introduces extra latency. Some other works use dedicated cores as a virtual switch~\cite{huang2017high}, which limits multi-core scalability.

%For inter-server communication, most works leverage a user-space stack~\cite{dunkels2001design,jeong2014mtcp,libvma,openonload} to achieve kernel bypass, but the CPU still needs to handle reliable transport. As RDMA becomes widely available in data centers, we hope to offload the transport to RDMA NICs when the peer supports RDMA. Furthermore, most works assume only one connection per pair of processes. However, load balancers, web servers and application gateways serve many concurrent connections~\cite{nishtala2013scaling,lin2016scalable,belay2017ix}. In light of this, both connection setup, event notification and data transmission under high concurrency need to be efficient.

%To demonstrate high performance, most existing works propose new abstractions for inter-process and inter-server communication.  existing socket applications need modifications to use the new abstractions. Furthermore, these stacks are not optimized for a large number of concurrent or short-lived connections, which is an important workload to serve Internet users and large distributed systems.

%One line of research optimize the kernel code or design user-space compatible stacks for higher socket performance. The kernel optimization approach does not eliminate context switch overhead, while system call batching introduces extra latency. User-space stacks are mostly designed for inter-server connections. With the trend of containerized micro-services, we expect an increasing number of applications or containers to be hosted on each server, where inter-process communication (IPC) inside server has more significance.

%To simplify deployment, we hope to accelerate existing applications without modification to the code. \textit{Socket compatibility} adds another dimension of challenge. The socket interface was designed for networking and IPC in millisecond scale, when memory copy, context switch, synchronization, cache miss and cache migration were considered inexpensive~\cite{barroso2017attack,belay2017ix}. An efficient socket architecture for microsecond-scale networking and IPC requires minimizing all overheads above. The semantics lead to challenges. First, the send buffer can be modified by application after non-blocking \texttt{send}, and the receive buffer is not determined until application calls \texttt{recv}. Data copy on \texttt{send} and \texttt{recv} seems mandatory. Second, connections are shared by processes and threads after \texttt{fork} and thread creation. It is challenging to avoid synchronization in this multi-producer and multi-consumer FIFO model. Third, multiple processes listening on a same IP and port compete for incoming connections.

%\textbf{The above part is motivation and related work.}

We design \sys{}, a high performance user-space socket architecture that is compatible with existing applications and preserves isolation among processes, while be scalable to multiple cores and many concurrent connections. The major component of \sys{} is a user-space library \libipc{} that intercepts and implements Linux socket APIs. We leverage different communication options to achieve performance that is close to physical limits of the underlying hardware. For inter-process connections, we use peer-to-peer shared memory in user space without going deep into the kernel or NIC. For intra-datacenter connections, we offload the transport to RDMA NICs. In addition, we use a lightweight user-space TCP/IP stack~\cite{dunkels2001design} to communicate with hosts without RDMA support.

\RED{Bojie: need rewrite below.}

The key to multi-core scalability and inter-process isolation is avoiding synchronization and state sharing. We \textit{consider processes as a distributed system} that communicates via message passing. To avoid inter-thread synchronization, each thread is regarded as a separate process. To maximize message passing performance, we design a \textit{lockless shared-memory queue} between each pair of processes. We avoid synchronization and atomic operations with shared-memory ring buffer inside server and one-sided RDMA \texttt{write} across data center network.

We further minimize message passing by partitioning socket states to each process. We implement the socket APIs \textit{as locally as possible}. In particular, we allocate file descriptors, buffers and manage socket options in the caller process. For non-local operations, we make the best effort to \textit{minimize centralized coordination and blocking wait}. \texttt{Send}, \texttt{recv} and \texttt{close} are implemented as non-blocking accesses to peer-to-peer shared memory queues. During process initialization and socket connection setup, we delegate privileged and non-partitionable operations to a centralized coordinator process --- the \textit{monitor}. Delegation is more secure and efficient than synchronization~\cite{roghanchi2017ffwd}.

To handle many concurrent connections efficiently, we need to save memory footprint and improve locality of memory accesses. Observing the event-driven behavior of applications, for each pair of processes, we \textit{multiplex socket connections through one shared memory queue or RDMA connection}. %To minimize inter-core interrupts, we use cooperative multitasking and implement event polling (\texttt{epoll}) in receiver. %Consequently, event notification (\texttt{epoll}) becomes process local when the process is not sleeping.

To solve the multi-producer multi-consumer challenge, we \textit{optimize for the common case and prepare for the worst case}. Senders transmit data via different queues in parallel. To ensure receiver ordering, based on the observation that applications seldom receive concurrently from a shared socket, the sender designates a receiver with exclusive access. We further develop mechanisms to avoid deadlock and starvation, in addition to handling unconsumed buffers during \texttt{fork} and thread creation.

To avoid memory copy of large buffers, we extend the \textit{page remapping} approach~\cite{thadani1995efficient,chu1996zero}, which enables copy-on-write upon \texttt{send} and remaps the send buffer to receiver's virtual address upon \texttt{recv}. First, we intercept \texttt{memcpy} of full pages to reduce copy-on-write. Second, we move kernel-based page allocation to user-space while preserving security. Finally, we leverage NIC features to achieve zero copy for both shared memory, RDMA and TCP/IP transports.

On the latency side, \sys{} achieves $0.25\mu$s RTT for intra-server socket, 60x faster than Linux and only $0.05\mu$s higher than a bare-metal shared memory queue. For inter-server socket, \sys{} achieves $2\mu$s RTT between RDMA hosts, almost the same as raw RDMA verbs and reduces Linux RTT by 10x. The RTT to a TCP/IP peer is $5\mu$s. On the throughput side, every second, a single-thread process can send 11~M intra-server messages (4x Linux), 8~M messages via RDMA (24x Linux) or 5~M messages via TCP/IP (15x Linux). Each thread can establish up to 1~M new connections per second (40x Linux). It is worth noting that the throughput is scalable with number of cores and connections.

We evaluate end-to-end performance of \sys{} using two categories of applications: \textit{network functions} and \textit{web services}. For a multi-core pipelined network function chain, a socket application accelerated by \sys{} achieves comparable performance than a state-of-the-art framework specialized for network functions~\cite{panda2016netbricks}. We also evaluate \sys{} on a standard web service composed of Nginx load balancer, Node.js web applications and memcached key-value store. For simple requests in short-lived connections, the web service serves 2~M requests per second (10x Linux). For an HTTP request that involves multi-round-trip key-value store accesses, \sys{} reduces end-to-end latency from 2000~ms to 50~ms. For a connection sending a large in-memory file, Nginx achieves 5x throughput and saturates the 100~Gbps NIC.
