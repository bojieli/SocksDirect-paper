\section{Introduction}
\label{sec:intro}

Most cloud applications use the socket API for inter-process communication among components or containers inside a same server and across data center network, in addition to serving Internet users. The overhead of Linux socket becomes salient given the rapid growth of network speed and number of CPU cores per server. We benchmark communication intensive applications (\textit{e.g.} Nginx and memcached) and find that 70\% to 90\% of CPU time is spent in the OS kernel. When more CPU cores are utilized, they even spend a larger portion of time in the kernel~\cite{boyd2010analysis}. In addition to CPU overhead, latency is also a problem. The round-trip time (RTT) between two processes communicating with shared memory can be as low as 0.2$\mu$s, while TCP socket RTT is above 10$\mu$s. In a data center, the RTT between RDMA servers is also one order of magnitude lower than kernel TCP/IP stack. In this regard, an efficient socket implementation should leverage different communication options for intra-server, intra-datacenter and WAN sockets.

There has been extensive work aiming to release the bare metal performance of multi-core CPU and data center network. For intra-server communication, there are mainly three lines of research. The first category of work use the NIC as a switch, but going deep to the NIC introduces $\approx2 \mu$s delay due to PCIe latency, one order of magnitude higher than shared memory. A second line of work use dedicated cores as a virtual switch, which limits multi-core scalability. Some other works optimize or redesign the kernel socket stack, where the kernel uses peer-to-peer shared memory communication among cores. However, this approach does not eliminate context switch overhead, while system call batching introduces extra latency.

For inter-server communication, most works leverage a user-space stack to achieve kernel bypass, but the CPU still needs to handle reliable transport. As RDMA becomes widely available in data centers, we hope to offload the transport to RDMA NICs. Furthermore, most works assume only one connection per pair of processes. However, load balancers, web servers and application gateways serve many concurrent connections. In light of this, both connection setup and data transmission under high concurrency need to be efficient.

%To demonstrate high performance, most existing works propose new abstractions for inter-process and inter-server communication.  existing socket applications need modifications to use the new abstractions. Furthermore, these stacks are not optimized for a large number of concurrent or short-lived connections, which is an important workload to serve Internet users and large distributed systems.

%One line of research optimize the kernel code or design user-space compatible stacks for higher socket performance. The kernel optimization approach does not eliminate context switch overhead, while system call batching introduces extra latency. User-space stacks are mostly designed for inter-server connections. With the trend of containerized micro-services, we expect an increasing number of applications or containers to be hosted on each server, where inter-process communication (IPC) inside server has more significance.


 


We design \sys{}, a high performance socket architecture that is compatible with existing applications, while scalable to many cores and connections. The semantics of socket was designed for networking and IPC in millisecond scale, when memory copy, context switch, synchronization, cache miss and cache migration were all considered inexpensive~\cite{barroso2017attack}. An efficient socket architecture for microsecond-scale networking and IPC requires minimizing all overheads above, while preserving the semantics.

To minimize context switch, we implement most of \sys{} in a user-space library \libipc{} that intercepts Linux APIs and implements socket operations. Similar to other user-space socket designs, accelerating socket is as simple as loading \libipc{} for existing applications. Since communication intensive applications are mostly event driven, we use cooperative multitasking and polling to minimize inter-core interrupts.

The key to multi-core scalability is avoiding synchronization and state sharing. We regard processes as a peer-to-peer system that uses message passing for IPC. Each thread is regarded as a separate process. To maximize message passing performance, we design a high performance shared-memory queue between each pair of processes. We further minimize message passing by partitioning socket states to each process. In particular, we allocate file descriptors and maintain socket options locally. For data transmission, sender uses non-blocking shared-memory write for IPC and one-sided RDMA \texttt{write} among servers, while the receiver 

connection is shared by processes and threads after \texttt{fork} and thread creation. We build a scalable architecture for the multi-producer and multi-consumer FIFO model.

To handle a large number of processes efficiently, we need to save memory footprint and improve locality of memory accesses. To this end, we multiplex

Zero copy.

Use RDMA efficiently. Light-weight TCP/IP stacks.
