\section{Implementation}
\label{sec:implementation}

For implementation, \libipc is divided to two parts: monitor and userspace library. Both parts are implemented in $\approx$5000 lines of C/C++ code. We take advantage of C++ templates for different types of queues in our design. %Specifically, we have two techniques to highlight:
In this section, we highlight several techniques in implementation:

%\subsection{Seamless system call hook}
%\label{subsec:syscall-hook}

\textbf{LD\_PRELOAD to intercept Linux APIs.}
In order to support existing applications seamlessly, \libipc leverages \textit{LD\_PRELOAD} environment variable in Linux to load a shared library to applications, which replace the function of GNU LibC which are the wrappers of the system call. 

\textbf{Multiplex FD between kernel and \libipc{}.}
In our system, we use the range of file descriptor number to identify whether the system call should be hook to \libipc to avoid problems related to file descriptors not related to socket. Taking the idea of LOS~\cite{huang2017high}, in \libipc, FD number is allocated in a top-down way i.e. from $2^{31}-1$ while Linux assign the number from bottom to up i.e. from zero. We set $2^{30} - 1 $ as the threshold to distinguish whether the system call ought to be handled by \libipc.

\textbf{Multiplex events between kernel and \libipc{}.}
The FD set of \texttt{epoll} may include both sockets and other \textit{kernel FDs} handled by Linux kernel.
%LOS~\cite{huang2017high} periodically invokes non-blocking \texttt{epoll\_wait} syscall with kernel FDs, which leads to a trade-off between delay and CPU overhead. Differently,
\libipc{} creates a per-process \textit{epoll thread} which runs an infinite loop of blocking \texttt{epoll\_wait} syscall with kernel FDs. Whenever epoll thread receives a kernel event, it broadcasts the event to application threads via shared memory queues. The \texttt{epoll\_wait} function in \libipc{} will return such kernel events in addition to socket events. Note that Linux allows an event to be received by multiple threads sharing the FD.

%\textbf{Accelerate access to local storage.}
%Use SPDK and user-mode file system (cite). How to multiplex processes in accessing a file system? (1) directory and metadata go to monitor, (2) read/write within allocated area of a file: process self, (3) append or read/write outside allocated area: handled by master process of a shared file. Monitor pre-allocate free blocks to processes (batch allocation and free).

\section{Discussion}
\label{sec:discussion}

\textbf{The ideal socket API.} \sys{} naturally supports message passing. Zero copy.

\textbf{Limitations of RDMA.} First, RDMA is not suitable for WAN. Second, RDMA has scalability issue when one server connects to many servers. Software transport in CPU access connection states in host memory, while hardware RDMA transport caches connection states in NIC and swaps out to host memory when cache overflows. First, CPU cache miss costs less than 0.1$\mu$s, while NIC cache miss costs 0.5$\mu$s~\cite{kaminsky2016design}. Second, CPU memory bandwidth is an order of magnitude larger than NIC PCIe bandwidth. In light of this, a host should switch to software transport when it actively communicates with a large number of hosts. Fortunately, Modern NICs has an increasing size of memory and supports more active connections without performance degradation~\cite{kaminsky2016design}.

\textbf{Multiple sender ordering.}
Ordering of multiple senders (linux is sequentially consistent).

\textbf{Remaining compatibility problems.}
Applications that do not use GNU libc.
Linux AIO and Windows I/O completion ports.
/proc fs.