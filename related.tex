\section{Related Work}
\label{sec:related}

There has been extensive work aiming to release the bare metal performance of multi-core CPU and data center network. Table~\ref{tab:related-work} categorizes and compares several representative existing approaches.

\parab{Linux kernel optimization.}
One line of research optimize the kernel code for higher socket performance, for example FastSocket~\cite{lin2016scalable}.
First, the kernel optimization approach does not eliminate kernel crossing overhead, while system call batching introduces extra latency.
Second, the socket interface still needs memory copy.
Third, for many concurrent connections, the kernel requires large memory footprint and thus may introduce cache misses.


\parab{New OS stacks.}
Another line of research propose kernel stacks with new interfaces.
Arrakis~\cite{peter2016arrakis} and IX~\cite{belay2017ix} use NIC for both intra-server and inter-server communications. The hairpin latency from CPU to NIC is at least two PCIe delays, which is roughly 1$\mu$s~\cite{kaminsky2016design}, one order of magnitude higher than inter-core communication delay ($\leq0.1\mu$s). In addition, the data plane switching capacity of a NIC is constrained by PCIe bandwidth, an order of magnitude lower than the bus among CPU cores~\cite{li2017kv}. For inter-server communication, IX~\cite{belay2017ix} implement transport in software, while Arrakis~\cite{peter2016arrakis} offload the transport to NIC. IX~\cite{belay2017ix} and NetKernel~\cite{niu2017network} enforce SLA in the kernel or hypervisor. Although software has more flexibility than the hardware-based transport in NIC, NICs are becoming increasingly programmable, and the architecture of programmable NICs is a better fit for implementing transport protocols~\cite{kaufmann2015flexnic,smartnic,mellanox,cavium}. 
Stackmap~\cite{yasukata2016stackmap} reuses the TCP/IP stack in Linux kernel to achieve protocol compatibility, but the abstraction for applications are changed to support zero copy.

Although these new stacks demonstrate high performance, existing socket applications need modifications to use the new abstractions.
Furthermore, for large scale deployment, kernel-based stacks are much more complicated than user-space libraries~\cite{andromeda}.

\parab{User-space socket.}
A third line of approach is user-space sockets.
LibVMA~\cite{libvma} is fully compatible with existing applications, but not scalable to multiple threads and connections.
mTCP~\cite{jeong2014mtcp} is compatible with most Linux socket functions, which uses batched function calls to use a packet-level interface of the NIC.
Additionally, most user-space sockets focus on inter-server and do not optimize for intra-server connections.

FreeFlow~\cite{freeflow} leverages shared memory for intra-server communication and RDMA for inter-server, but it provides an RDMA interface.
Existing socket to RDMA translation approaches, \textit{e.g.} SDP~\cite{socketsdirect} and rsockets~\cite{rsockets} are not compatible with Linux socket because the application still needs to manage send and receive buffers.


%\parab{RDMA.}
%First, RDMA is not suitable for WAN. Second, RDMA has scalability issue when one server connects to many servers. Software transport in CPU access connection states in host memory, while hardware RDMA transport caches connection states in NIC and swaps out to host memory when cache overflows. First, CPU cache miss costs less than 0.1$\mu$s, while NIC cache miss costs 0.5$\mu$s~\cite{kaminsky2016design}. Second, CPU memory bandwidth is an order of magnitude larger than NIC PCIe bandwidth. In light of this, a host should switch to software transport when it actively communicates with a large number of hosts. Fortunately, Modern NICs has an increasing size of memory and supports more active connections without performance degradation~\cite{kaminsky2016design}.




\iffalse
Kernel-bypass TCP/IPs
IX [OSDI’14], Arrakis [OSDI’14], UTCP [CCR’14], Sandstorm [SIGCOMM’14], mTCP [NSDI’14], Seastar

Socket API enhancements
MegaPipe [OSDI’12], FlexSC [OSDI’10], KCM [Linux]

Improving OS stack with fast packet I/O
mSwitch [SOSR’15]

In-stack improvement
FastSocket [ASPLOS’16]

Running kernel stack in user-space
Rump [AsiaBSDCon’09], NUSE [netdev’15]
\fi



\iffalse
\begin{itemize}
	\item New abstraction (RDMA, lwip + DPDK etc.) 
	\begin{itemize}
		\item 
	\end{itemize}
	\item Compatible with socket (libvma, LOS etc.) 
	\begin{itemize}
		\item Violate goal 2: memory copy 
		\item Violate goal 3: thread synchronization for multi-thread applications 
	\end{itemize}
	\item Common problems: 
	\begin{itemize}
		\item Designed for networking, does not support or optimize for IPC communication inside the same server 
		\item Violate goal 4: Not optimized for many connections 
	\end{itemize}
\end{itemize}
\fi

