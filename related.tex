\section{Related Work}
\label{sec:related}


Table~\ref{tab:related-work} categorizes and compares several representative existing approaches.

\textbf{Linux kernel optimization.}
FastSocket~\cite{lin2016scalable}, 

\begin{itemize}
	\item
	Violate goal 1: inter-core interrupts for IPC 
	\item
	Violate goal 2: context switch overhead, +latency for syscall batching, still need memory copy 
	\item 
	Violate goal 4: Large memory footprint for many connections 
\end{itemize}

\textbf{New kernel stacks.}


\iffalse
Kernel-bypass TCP/IPs
IX [OSDI’14], Arrakis [OSDI’14], UTCP [CCR’14], Sandstorm [SIGCOMM’14], mTCP [NSDI’14], Seastar

Socket API enhancements
MegaPipe [OSDI’12], FlexSC [OSDI’10], KCM [Linux]

Improving OS stack with fast packet I/O
mSwitch [SOSR’15]

In-stack improvement
FastSocket [ASPLOS’16]

Running kernel stack in user-space
Rump [AsiaBSDCon’09], NUSE [netdev’15]
\fi



Arrakis~\cite{peter2016arrakis} and IX~\cite{belay2017ix} use NIC for both intra-server and inter-server communications. The hairpin latency from CPU to NIC is at least two PCIe delays, which is roughly 1$\mu$s~\cite{kaminsky2016design}, one order of magnitude higher than inter-core communication delay ($\leq0.1\mu$s). In addition, the data plane switching capacity of a NIC is constrained by PCIe bandwidth, an order of magnitude lower than the bus among CPU cores~\cite{li2017kv}. For inter-server communication, Arrakis~\cite{peter2016arrakis} and IX~\cite{belay2017ix} implement transport in software. Although this has more flexibility than the hardware-based transport in RDMA, NICs are becoming increasingly programmable, and the architecture of programmable NICs is a better fit for implementing transport protocols~\cite{kaufmann2015flexnic,smartnic,mellanox,cavium}. 
IX~\cite{belay2017ix} and NetKernel~\cite{niu2017network} implement stacks in the kernel or hypervisor to enforce SLA.


Arrakis reaches for utmost performance by relying on hardware to enforce per-application maximum I/O rates and allowed communication peers. IX trades performance for software control over network I/O, thus allowing the precise enforcement of the I/O behavior of a particular network protocol, such as TCP congestion control.~\cite{bailis2016introducing}

For large scale deployment, kernel-based stacks are much more complicated than user-space libraries~\cite{andromeda}.

\textbf{User-space sockets.}
Seastar~\cite{seastar}, F-stack~\cite{fstack}


\begin{itemize}
	\item New abstraction (RDMA, lwip + DPDK etc.) 
	\begin{itemize}
		\item Some works violate goal 3: data transmission via centralized virtual switch 
	\end{itemize}
	\item Compatible with socket (libvma, LOS etc.) 
	\begin{itemize}
		\item Violate goal 2: memory copy 
		\item Violate goal 3: thread synchronization for multi-thread applications 
	\end{itemize}
	\item Common problems: 
	\begin{itemize}
		\item Designed for networking, does not support or optimize for IPC communication inside the same server 
		\item Violate goal 4: Not optimized for many connections 
	\end{itemize}
\end{itemize}


SDP~\cite{socketsdirect} and rsockets~\cite{rsockets} are not compatible with Linux socket because the application still needs to manage send and receive buffers.



PacketShader~\cite{han2010packetshader}
F-stack~\cite{fstack}
DPDK~\cite{dpdk} lwip~\cite{dunkels2001design}
PF\_RING~\cite{pf-ring}
netmap~\cite{rizzo2012netmap}

Improving network connection locality in multi-core:
Affinity-Accept~\cite{pesterev2012improving}


MegaPipe~\cite{han2012megapipe}

system call scheduling:
FlexSC~\cite{soares2010flexsc}

Vector operating systems.
VOS~\cite{vasudevan2011case}

mTCP~\cite{jeong2014mtcp}
StackMap~\cite{yasukata2016stackmap}

data center RPC~\cite{stuedi2014darpc}

Network specialization for performance~\cite{marinos2014network}

OpenOnload~\cite{openonload}
TCP chimney offload~\cite{networking2004network}
SeaStar~\cite{seastar}

Dune: safe user-mode access to privileged CPU features~\cite{belay2012dune}

Scalable commutative rule~\cite{clements2015scalable}

Unikernel~\cite{madhavapeddy2013unikernels}

libevent~\cite{libevent}

Metron NFV NSDI'18~\cite{metron2018nsdi}

Anna~\cite{anna}

Linux API what needs to be supported~\cite{tsai2016study}

MICA~\cite{lim2014mica}

FARM~\cite{dragojevic2014farm}
One-sided RDMA~\cite{mitchell2013using}
Two-sided RDMA~\cite{kalia2014using}
RDMA design guidelines~\cite{kaminsky2016design}
LITE kernel RDMA~\cite{tsai2017lite}

FreeFlow~\cite{freeflow}


ffwd delegation~\cite{roghanchi2017ffwd}

Mellanox VMA~\cite{libvma}, Solarflare OpenOnload~\cite{openonload} and Myricom DBL~\cite{dbl}.

Zero copy socket via \textit{page remapping} has been implemented in UNIX~\cite{thadani1995efficient} and BSD~\cite{chu1996zero}. 
