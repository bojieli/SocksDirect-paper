\section{Related Work}
\label{sec:related}


\begin{table*}[t]
	\centering
	\scalebox{0.9}{
		\begin{tabular}{l|c|ccc|cc|ccc|}
			\hline
			Category	& \multicolumn{1}{c|}{Linux Opt.} & \multicolumn{3}{c|}{New Kernel Stack} & \multicolumn{2}{c|}{User-space Packet} & \multicolumn{3}{c|}{User-space Socket} \\
			\hline
			System	& FastSocket & IX & MegaPipe & StackMap & Arrakis & FreeFlow & mTCP & libvma & SocksDirect \\
			\hline
			\hline
			Kernel Bypass & & & & & \yes & \yes & \yes & \yes & \yes \\
			\hline
			Zero Copy & & \yes & & \yes & \yes & \yes & & & \yes \\
			\hline
			Low Latency & & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes \\
			\hline
			\hline
			Multi-core Scalability & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & \yes \\
			\hline
			Connection Scalability & \yes & \yes & \yes & \yes & & & \yes & & \yes \\
			\hline
			\hline
			NIC-bypass IPC & \yes & & \yes & \yes & & \yes & \yes & & \yes \\
			\hline
			RDMA as Transport & & & & & & \yes & & & \yes \\
			\hline
			\hline
			Socket-like API & \yes & & \yes & \yes & \yes & & \yes & \yes & \yes \\
			\hline
			Linux Compatible & \yes & & & & & & & \yes & \yes \\
			\hline
			Process Isolation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes \\
			\hline
		\end{tabular}
	}
	\caption{Comparison of high performance socket systems.}
	\label{tab:related-work}
\end{table*}

Table~\ref{tab:related-work} categorizes and compares several representative existing approaches.

\textbf{Linux kernel optimization.}
FastSocket~\cite{lin2016scalable}, 

\begin{itemize}
	\item
	Violate goal 1: inter-core interrupts for IPC 
	\item
	Violate goal 2: context switch overhead, +latency for syscall batching, still need memory copy 
	\item 
	Violate goal 4: Large memory footprint for many connections 
\end{itemize}

\textbf{New kernel stacks.}


\iffalse
Kernel-bypass TCP/IPs
IX [OSDI’14], Arrakis [OSDI’14], UTCP [CCR’14], Sandstorm [SIGCOMM’14], mTCP [NSDI’14], Seastar

Socket API enhancements
MegaPipe [OSDI’12], FlexSC [OSDI’10], KCM [Linux]

Improving OS stack with fast packet I/O
mSwitch [SOSR’15]

In-stack improvement
FastSocket [ASPLOS’16]

Running kernel stack in user-space
Rump [AsiaBSDCon’09], NUSE [netdev’15]
\fi

Arrakis~\cite{peter2016arrakis} and IX~\cite{belay2017ix} use NIC for both intra-server and inter-server communications. The hairpin latency from CPU to NIC is at least two PCIe delays, which is roughly 1$\mu$s~\cite{kaminsky2016design}, one order of magnitude higher than inter-core communication delay ($\leq0.1\mu$s). In addition, the data plane switching capacity of a NIC is constrained by PCIe bandwidth, an order of magnitude lower than the bus among CPU cores~\cite{li2017kv}. For inter-server communication, Arrakis~\cite{peter2016arrakis} and IX~\cite{belay2017ix} use packet-level interface of NIC and implement transport in software. Although this has more flexibility than the hardware-based transport in RDMA, NICs are becoming increasingly programmable, and the architecture of programmable NICs is a better fit for implementing transport protocols~\cite{kaufmann2015flexnic,smartnic,mellanox,cavium}. 

For large scale deployment, kernel-based stacks are much more complicated than user-space libraries~\cite{andromeda}.

\textbf{User-space sockets.}
Seastar~\cite{seastar}, F-stack~\cite{fstack}


\begin{itemize}
	\item New abstraction (RDMA, lwip + DPDK etc.) 
	\begin{itemize}
		\item Some works violate goal 3: data transmission via centralized virtual switch 
	\end{itemize}
	\item Compatible with socket (libvma, LOS etc.) 
	\begin{itemize}
		\item Violate goal 2: memory copy 
		\item Violate goal 3: thread synchronization for multi-thread applications 
	\end{itemize}
	\item Common problems: 
	\begin{itemize}
		\item Designed for networking, does not support or optimize for IPC communication inside the same server 
		\item Violate goal 4: Not optimized for many connections 
	\end{itemize}
\end{itemize}


SDP~\cite{socketsdirect} and rsockets~\cite{rsockets} are not compatible with Linux socket because the application still needs to manage send and receive buffers.



PacketShader~\cite{han2010packetshader}
F-stack~\cite{fstack}
DPDK~\cite{dpdk} lwip~\cite{dunkels2001design}
PF\_RING~\cite{pf-ring}
netmap~\cite{rizzo2012netmap}

Improving network connection locality in multi-core:
Affinity-Accept~\cite{pesterev2012improving}


MegaPipe~\cite{han2012megapipe}

system call scheduling:
FlexSC~\cite{soares2010flexsc}

Vector operating systems.
VOS~\cite{vasudevan2011case}

mTCP~\cite{jeong2014mtcp}
StackMap~\cite{yasukata2016stackmap}

data center RPC~\cite{stuedi2014darpc}

Network specialization for performance~\cite{marinos2014network}

OpenOnload~\cite{openonload}
TCP chimney offload~\cite{networking2004network}
SeaStar~\cite{seastar}

Dune: safe user-mode access to privileged CPU features~\cite{belay2012dune}

Scalable commutative rule~\cite{clements2015scalable}

Unikernel~\cite{madhavapeddy2013unikernels}

libevent~\cite{libevent}

Metron NFV NSDI'18~\cite{metron2018nsdi}

Anna~\cite{anna}

Linux API what needs to be supported~\cite{tsai2016study}

MICA~\cite{lim2014mica}

FARM~\cite{dragojevic2014farm}
One-sided RDMA~\cite{mitchell2013using}
Two-sided RDMA~\cite{kalia2014using}
RDMA design guidelines~\cite{kaminsky2016design}
LITE kernel RDMA~\cite{tsai2017lite}

FreeFlow~\cite{freeflow}


ffwd delegation~\cite{roghanchi2017ffwd}

Mellanox VMA~\cite{libvma}, Solarflare OpenOnload~\cite{openonload} and Myricom DBL~\cite{dbl}.

Zero copy socket via \textit{page remapping} has been implemented in UNIX~\cite{thadani1995efficient} and BSD~\cite{chu1996zero}. 

%dIPC~\cite{vilanova2017direct}

Recent years we see a trend of specializing operating system for high performance.
The network stack in an operating system can be roughly divided into two layers: network packet processing and application interface (\textit{e.g.} socket, RDMA).

To accelerate network packet processing,
FlexNIC~\cite{kaufmann2015flexnic} redesigns NIC architecture to support flexible and high-performance processing.
E2~\cite{palkar2015e2} and NetBricks~\cite{panda2016netbricks} are high performance packet processing systems using a pipeline of dedicated cores.
Corey~\cite{boyd2008corey} and Multikernel~\cite{baumann2009multikernel} achieve multicore scalability by replacing shared memory coordination with explicit message passing among cores.
FreeFlow~\cite{freeflow} achieves high performance container networking for long-lived connections using user-space communication.
Inter-core message passing has limited efficiency. To receive a message from a lockless shared-memory queue, the receiver core needs to read non-cached data, because the cache has been invalidated by the message sender. There is no instruction to directly copy data from one core's private cache to another core's.

To translate application interface to network packets efficiently, one line of work develops lightweight TCP/IP stacks to provide socket-compatible APIs with high performance.
Netmap~\cite{rizzo2012netmap} and VALE~\cite{rizzo2012vale} are high-performance user-space network stacks with BSD-like API, but do not integrate in the Linux file descriptor namespace.
Fastsocket~\cite{lin2016scalable} and F-stack~\cite{fstack} improves Linux socket performance and scalability while keeping the most extent compatibility.
However, many Linux APIs are rarely used~\cite{tsai2016study} and they constitute a large portion of processing delay on the data path~\cite{peter2016arrakis}.
TCP Chimney~\cite{networking2004network} offloads a part of network stack to hardware.

The other line of work uses hardware (the NIC) to achieve kernel-bypass networking between VMs or processes on the same host.
Arrakis~\cite{peter2016arrakis} and IX~\cite{belay2017ix} leverage hardware-assisted virtualization and SR-IOV NICs to bypass kernel coordination.
Mellanox VMA~\cite{libvma} translates socket APIs into RDMA verbs via a light-weight socket and network stack in user space.
When applying these designs to container networking, there are two challenges: (1) All packets traverse through the NIC, while the NIC has limited \textit{hairpin} processing capacity and PCIe bandwidth. (2) The number of NIC virtual functions (VF) is not enough to support each container with a VF.
This paper basically falls in this category and solves the challenges using careful hardware-software co-design.

After years of broken promises, the increasing gap between network speed and CPU processing capacity finally makes \textit{programmable NICs} deployed in data centers~\cite{smartnic}, opening up new possibilities for network stack design.

Why not use NIC:
Finally, for inter-process communication in a same server, RDMA still needs the NIC to copy from send to receive buffer. Because the PCIe bus between NIC and CPU has an order of magnitude lower bandwidth and higher latency than the DDR channels between memory and CPU~\cite{li2017kv}, it is desirable to bypass NIC for inter-process communication.
