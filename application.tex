\section{Applications}
\label{sec:application}

In this section, we demonstrate that \sys{} can significantly improve the performance of real-world applications without modifying the code.
For web applications, we conduct experiment for a widely used scenario: One load balancer is connected with the backend web service, while the web service accesses the key value store for multiple round-trips.

Figure \ref{fig:eval-nginx-multiround} shows the end-to-end latency with different number of round-trips between the web service and key value store. As we measured, the Linux socket takes 69\% of the total CPU time before using \libipc. Since the intra-server latency of \sys{} is 30x lower than Linux socket, the end-to-end latency of the system with \sys{}  is 1/3 of that of original Linux socket.



%\subsection{Network Function}

%Network Function Virtualization is prevalent and widely used in modern datacenters. We build up a chain of Network Functions (NF) with different lengths in one host. Each NF in the chain runs as a process and is pinned on one CPU core. It receives packets from the predecessor, modifies one byte of each packet, then sends them to the successor. We compare the performance of \sys{} with Linux socket and NetBricks~\cite{panda2016netbricks}, a highly optimized NF framework. Figure~\ref{fig:eval-tun-tput} shows that our \sys achieves performance that is comparable with NetBricks and scalable with number of cores.



%\subsection{Web Application}


%, Nodejs~\cite{nodejs} and memcached~\cite{memcached}

%\begin{itemize}
%	\item Figure~\ref{fig:eval-nginx-short}: Many short-lived connections. %Nginx $\rightarrow$ Nodejs, Nodejs access memcached once.
%	\item Figure~\ref{fig:eval-nginx-multiround}: Each connection, backend %interact extensively. (Nodejs access memcached multiple round trips.)
%\end{itemize}


%\subsection{Real-time Stream Processing}

%Apache Flink~\cite{carbone2015apache} (need to turn off durability on disk)

%Scenario: Word Count (distributed system with one source, two mappers and one reducer)

%Metrics: Latency, throughput

%\subsection{Machine Learning}

%Tensorflow~\cite{abadi2016tensorflow}

%Scenario: (Distributed Tensorflow) Parameter server and worker on a same server.

%Metrics: Time per iteration