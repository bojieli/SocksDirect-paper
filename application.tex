\section{Applications}
\label{sec:application}



\begin{table*}[t]
	\centering
	\scalebox{0.9}{
		\begin{tabular}{l|c|ccc|cc|ccc|}
			\hline
			Category	& \multicolumn{1}{c|}{Linux Opt.} & \multicolumn{3}{c|}{New Kernel Stack} & \multicolumn{2}{c|}{User-space Packet} & \multicolumn{3}{c|}{User-space Socket} \\
			\hline
			System	& FastSocket & IX & MegaPipe & StackMap & Arrakis & FreeFlow & mTCP & libvma & SocksDirect \\
			\hline
			\hline
			Kernel Bypass & & & & & \yes & \yes & \yes & \yes & \yes \\
			\hline
			Zero Copy & & \yes & & \yes & \yes & \yes & & & \yes \\
			\hline
			Low Latency & & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes \\
			\hline
			\hline
			Multi-core Scalability & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & \yes \\
			\hline
			Connection Scalability & \yes & \yes & \yes & \yes & & & \yes & & \yes \\
			\hline
			\hline
			NIC-bypass IPC & \yes & & \yes & \yes & & \yes & \yes & & \yes \\
			\hline
			RDMA as Transport & & & & & & \yes & & & \yes \\
			\hline
			\hline
			Socket-like API & \yes & & \yes & \yes & \yes & & \yes & \yes & \yes \\
			\hline
			Linux Compatible & \yes & & & & & & & \yes & \yes \\
			\hline
			Process Isolation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes \\
			\hline
		\end{tabular}
	}
	\caption{Comparison of high performance socket systems.}
	\label{tab:related-work}
	\vspace{-15pt}
\end{table*}

In this section, we demonstrate that \sys{} could significantly improve the performance of realistic applications without modifying the code.


\subsection{Network Function}

Network Function Virtualization is prevalent and widely used in modern datacenters. We build up chains of Network Functions (NF) with different lengths in one machine. Each NF in the chain runs as a process and occupies one CPU core. It receives the packet from the predecessor process in the chain, modifies one byte of the packet, then sends it to the successor process. We compare the performance of \sys{} with Linux socket and NetBricks~\cite{panda2016netbricks}, a highly optimized network function framework. Figure~\ref{fig:eval-tun-tput} shows that our \sys achieves performance that is comparable with NetBricks and scalable with number of cores.



\subsection{Web Application}
For web applications, we conduct experiment for a widely used scenario: One load balancer is connected with the backend web service, while the web service accesses the key value store for multiple round-trips.

Figure \ref{fig:eval-nginx-multiround} shows the end-to-end latency with different number of round-trips between the web service and key value store. As we measured, the Linux socket takes 69\% of the total CPU time before using \libipc. Since the intra-server latency of \sys{} is 30x lower than Linux socket, the end-to-end latency of the system with \sys{}  is 1/3 of that of original Linux socket.



%, Nodejs~\cite{nodejs} and memcached~\cite{memcached}

%\begin{itemize}
%	\item Figure~\ref{fig:eval-nginx-short}: Many short-lived connections. %Nginx $\rightarrow$ Nodejs, Nodejs access memcached once.
%	\item Figure~\ref{fig:eval-nginx-multiround}: Each connection, backend %interact extensively. (Nodejs access memcached multiple round trips.)
%\end{itemize}


%\subsection{Real-time Stream Processing}

%Apache Flink~\cite{carbone2015apache} (need to turn off durability on disk)

%Scenario: Word Count (distributed system with one source, two mappers and one reducer)

%Metrics: Latency, throughput

%\subsection{Machine Learning}

%Tensorflow~\cite{abadi2016tensorflow}

%Scenario: (Distributed Tensorflow) Parameter server and worker on a same server.

%Metrics: Time per iteration