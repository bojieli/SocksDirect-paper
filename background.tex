\section{Background}
\label{sec:background}

\subsection{Socket: the Performance Bottleneck}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Fraction of CPU time in the kernel (socket connection setup, socket data transmission and non-socket system calls) and user-mode applications.}
	\label{fig:socket-kernel-time}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Performance of back-end systems using inter-server socket, intra-server socket, inter-server RDMA and intra-server shared memory.}
	\label{fig:backend-performance}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Throughput (bar) and latency (line) for 16B messages using inter-server TCP socket, inter-server RDMA, intra-server TCP socket, UNIX socket, pipe and shared memory queue.}
	\label{fig:socket-comparison}
\end{figure}


Socket is a well-known bottleneck in fast data center networks \RED{(cite)}. In a data center, we categorize systems that process queries from Internet users to be \textit{front-end} (\textit{e.g.} DNS server, load balancer and Web service), and the other systems (\textit{e.g.} database, stream processing and machine learning) to be \textit{back-end}.
For front-end systems, as shown in Figure~\ref{fig:socket-kernel-time}, 70\% -- 90\% CPU time is used in socket system calls. They maintain a large number of socket connections to Internet users, so connection setup consumes a significant portion of CPU time~\cite{lin2016scalable}.
For back-end systems, as shown in Figure~\ref{fig:backend-performance}, the performance using socket is significantly worse than using RDMA or shared memory. This is because back-end systems interact frequently with other nodes and services, consequently the latency is vital for their performance. The socket latency between different processes in a same server is $\approx$10~$\mu$s, even much higher than inter-server RDMA latency ($\approx$2~$\mu$s) (Figure~\ref{fig:socket-comparison}).


There has been much work~\cite{peter2016arrakis,lin2016scalable} to analyze the source of socket processing overhead.

\textbf{Context switch.}
Socket APIs are implemented in kernel space and thus require context switch for each socket operation. To protect against side-channel attacks based on speculative execution~\cite{Kocher2018spectre} and caching~\cite{Lipp2018meltdown}, system calls become more expensive~\cite{kpti}.

\textbf{Network stack.}
In Linux socket, inter-server connections need to go through the TCP/IP stack, and receiving each packet takes $\approx$3$\mu$s CPU time~\cite{peter2016arrakis}. In addition, processes communicating within a same server still need to communicate through the network stack.

\textbf{Data copy.}
For inter-server data transmission, Linux kernel copies data four times: on the send side, from the application buffer to kernel socket buffer, then to NIC ring buffer; the receive side is similar.
For intra-server transmission, data is copied three times: from the send application to kernel send buffer, then to kernel receive buffer and finally to the receive application.
%However, each CPU core can only copy 5~GiB data per second, limiting throughput for applications that operate on large chunks of data~\cite{panda2016netbricks}.

\textbf{Synchronization.}
Scalability analysis of Linux system calls~\cite{boyd2010analysis} suggests that many socket operations are not commutable. The Linux kernel needs to acquire global locks for socket connection setup and a per-socket lock for each socket operation. The synchronization overhead greatly mitigates

\begin{itemize}
	\item Context switch
	\item Data copy
	\item OS network stack (overhead for intra-server \& does not leverage RDMA hardware for inter-server)
	\item Lock in kernel (CPU overhead \& not scalable)
\end{itemize}

Many works propose to use user-level TCP/IP stack \RED{(cite)} or RDMA \RED{(cite)} to improve




The Multikernel design: replaces locks in kernel with message passing, but does not remove context switch.

List related work on fast sockets. They are not compatible with existing applications.

\subsection{Challenges for High Performance Socket}
\label{subsec:challenges}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Latency comparison of mutual exclusion mechanisms (CAS, mutex, futex) and cache migration.}
	\label{fig:mutual-exclusion}
\end{figure}

\textbf{Kernel bypass.}
The common wisdom is that user-space network stacks do not provide multiplexing and isolation among applications, therefore dataplane operating systems either demonstrate good performance with kernel-based network stacks~\cite{belay2014ix,tsai2017lite} or delegate socket processing to a virtual switch running on dedicated cores~\cite{martins2014clickos,roghanchi2017ffwd}. \sys resolves the dilemma by leveraging CPU memory protection and NIC multi-queue to provide multiplexing and isolation.


\textbf{Mutual exclusion for shared socket.}
The main challenge for Linux socket scalability is that a socket connection may be shared by multiple processes and threads. Threads in a same process share all socket connections. When a process forks, the parent and child processes share the sockets created before fork. The Linux implementation uses a shared queue and protects concurrent operations with mutual exclusion.
Ideally, inter-process message passing requires only one cache migration. As shown in Figure~\ref{fig:mutual-exclusion}, the Compare-And-Swap (CAS) instruction for mutual exclusion, even in non-contended case, is significantly slower than cache migration. Kernel mutex and user-mode futex are even slower than CAS. Mutual exclusion becomes a bottleneck. In addition, both mutual exclusion and contended memory write are not scalable to multiple cores~\cite{boyd2014optimizing}.

Fortunately, we observe that most applications do not receive from a socket connection concurrently from multiple processes or threads. For such producer-consumer scenarios, applications typically use message queues~\cite{hintjens2013zeromq}. However, applications do implicitly transfer a socket from one receiving process or thread to another (\textit{e.g.} from master to a worker). Multiple threads may also send large chunks of data (\textit{e.g.} log) through a shared socket.
To this end, we need a scalable socket architecture to eliminate mutual exclusion in the most cases and ensure correctness in the worst case.


\textbf{Reduce per-socket memory footprint.}
Each process may connect to multiple Internet hosts and data center servers. In Linux, each socket connection has dedicated send and receive buffers, and each buffer is at least one page (4KB). With millions of concurrent connections, the socket buffers consumes tens of gigabytes of memory, most of which is empty.



\textbf{Zero copy.}
The socket API was initially designed for blocking operations. The \textit{reactive} semantics of Linux event polling API seems to mandate data copy for both send and receive. After a non-blocking send operation, the send buffer may be overwritten by the application, so the implementation of send() needs to copy the data to an internal buffer. For the receive operation, application provides a buffer and processes the data after recv() returns. If the data is buffered on the sender, the application needs to wait for a round-trip time on recv(). Otherwise, the receiver's network stack cannot predict the user-provided recv() address and needs to buffer received data internally.

On the contrary, zero-copy API (\textit{e.g.} RDMA) has \textit{proactive} semantics. The application manages buffers, reserves the send buffer after send operation and releases it after send completion notification. The application also delegates non-blocking receive operations along with receive buffers, so the NIC can DMA received data directly to the receive buffers, then notify the application that data is ready. Although rsockets~\cite{rsockets} and SDP~\cite{socketsdirect} simplify RDMA API, they are still more complicated than the Linux socket API because the application needs to manage the buffers and predict how much data it expects to receive. Furthermore, for inter-process communication in a same server, RDMA still needs the NIC to copy from send to receive buffer. Because the PCIe bus between NIC and CPU has an order of magnitude lower bandwidth and higher latency than the DDR channels between memory and CPU~\cite{li2017kv}, it is desirable to bypass NIC for inter-process communication.
