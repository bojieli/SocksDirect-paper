\section{Background}
\label{sec:background}

\subsection{Socket: the Performance Bottleneck}
\label{subsec:bottleneck}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Fraction of CPU time in the kernel (socket connection setup, socket data transmission and non-socket system calls) and user-mode applications.}
	\label{fig:socket-kernel-time}
\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Performance of back-end systems using inter-server socket, intra-server socket, inter-server RDMA and intra-server shared memory.}
%	\label{fig:backend-performance}
%\end{figure}


%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Latency comparison of mutual exclusion mechanisms (CAS, mutex, futex) and cache migration.}
%	\label{fig:mutual-exclusion}
%\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Throughput (bar) and latency (line) for 16B messages using inter-server TCP socket, inter-server RDMA, intra-server TCP socket, UNIX socket, pipe and shared memory queue.}
	\label{fig:socket-comparison}
\end{figure}

Linux kernel socket is a well-known bottleneck for communication intensive applications. We stress test Nginx load balancer, memcached and Redis key-value stores with one request per TCP connection or a stream of requests on pre-established TCP connections. As shown in Figure~\ref{fig:socket-kernel-time}, 50\%$\sim$90\% CPU time is consumed in the kernel, mostly dealing with socket system calls. Connection setup is especially slow, causing the one request per connection scenario shows much lower throughput. If we were able to mitigate the overhead associated with sockets, application performance would be 2x$\sim$10x. As another example, we replaced socket with RDMA and enabled zero copy gRPC in distributed Tensorflow~\cite{abadi2016tensorflow}, and training a VGG network shows 5x speedup.

%Socket is a well-known bottleneck in fast data center networks \RED{(cite)}. In a data center, we categorize systems that process queries from Internet users to be \textit{front-end} (\textit{e.g.} DNS server, load balancer and Web service), and the other systems (\textit{e.g.} database, stream processing and machine learning) to be \textit{back-end}.

%For front-end systems, as shown in Figure~\ref{fig:socket-kernel-time}, 70\% -- 90\% CPU time is used in socket system calls. They maintain a large number of socket connections to Internet users. Establishing a socket connection takes $\approx$40$\mu$s CPU time, so a CPU core can only serve 25K new connections per second~\cite{lin2016scalable}.

%For back-end systems, the performance using socket is significantly worse than using RDMA or shared memory. This is because back-end systems interact frequently with other nodes and services, consequently the latency is vital for their performance. As shown in Figure~\ref{fig:socket-comparison}, socket latency between different processes in a same server is $\approx$10~$\mu$s, even much higher than inter-server RDMA latency ($\approx$2~$\mu$s).

%\textbf{Event-driven programming.}

%\textbf{Socket process: connection setup and data transmission.}

The source of Linux socket overhead has been analyzed extensively~\cite{peter2016arrakis,lin2016scalable}.

\textbf{Context switch.}
Socket APIs are implemented in kernel space and thus require context switch for each socket operation. Recently, the KPTI patches~\cite{kpti} to protect against Spectre~\cite{Kocher2018spectre} and Meltdown~\cite{Lipp2018meltdown} attacks make system calls 4x expensive (from 48~ns to 180~ns on our server). Switching from one process to another cooperatively ($0.45\mu$s) is 2.5x expensive than a system call. Inter-process context switch based on semaphore, mutex or \texttt{futex} takes $2\sim4\mu$s, even 5x$\sim$8x slower than cooperative multitasking.

\textbf{Data copy.}
For inter-server data transmission, Linux kernel copies data four times: on the send side, from application to kernel socket buffer, then to NIC ring buffer; the receive side is similar. For intra-server, data is copied three times: from application to kernel send buffer, then to kernel receive buffer via loopback interface, and finally to another application. Each CPU core can copy $\approx$5~GiB data per second~\cite{panda2016netbricks}, that's why a kernel TCP connection is hard to saturate an 100~Gbps NIC.

\textbf{Synchronization.}
During connection setup, we need to create file descriptors, load balance \texttt{connect} requests and allocate buffers. These operations are easy to implement with serialized execution. Linux kernel acquires global locks during connection setup and a per-socket lock for each socket operation, limiting socket scalability~\cite{boyd2010analysis,han2012megapipe,lin2016scalable}. In fact, even in non-contending scenarios, synchronization still introduces significant overhead. Mutual exclusion is typically implemented with Compare-And-Swap (CAS) instruction, which costs $100\sim200$~ns per lock and unlock. In comparison, inter-core message passing can be implemented with only one cache migration~\cite{roghanchi2017ffwd} (30~ns).

\textbf{Virtual file system.}
Each socket connection is a \textit{file descriptor} (FD) in Linux. First, each new connection involves expensive \texttt{inode} and \texttt{dentry} creation in the \texttt{proc} file system, which is not scalable and rarely used except for diagnostic and monitoring utilities. Second, socket operations need to pass through the virtual file system (VFS), which introduces CPU overhead and additional latency.

\textbf{TCP Transport.}
Linux TCP implementation has evolved to be complicated over the years to address performance and security concerns, as well as incorporate extensions~\cite{yasukata2016stackmap}. However, intra-server sockets do not need the ordering guarantee, loss recovery and congestion control offered by TCP. For intra-datacenter communication, the RDMA stack implements equivalent functionalities in NIC hardware, and an emerging line of work~\cite{zhu2015congestion,guo2016rdma,lu2017memory,mprdma} improve RDMA performance in a large scale data center network. In this work, we consider RDMA a mature technology to replace TCP whenever possible.

\textbf{Memory footprint and cache miss.}
In Linux, each socket connection has dedicated send and receive buffers, and each buffer is at least one page (4~KiB). With one million concurrent connections, the socket buffers consumes tens of gigabytes of memory, most of which is empty. Accessing many buffers randomly also leads to non-cached memory accesses, which is slower than sequential access to a single buffer~\cite{li2017kv}. In addition, event-driven applications use \texttt{epoll} to know which connections are ready for send or receive. If the event queue is separated from data queues, each \texttt{recv} operation involves two cache migrations for event and data. In light of this, we need to reduce memory accesses per data transmission.


%For data transmission, modern commodity NICs support multiple queues with receive flow steering~\cite{herbertrfs,mellanox}, so connection multiplexing and isolation can be offloaded to the NIC. Our goal is to achieve multi-core scalability for both connection setup and data transmission.

%\textbf{Network stack.}
%Receiving each packet from NIC takes $\approx$3$\mu$s CPU time~\cite{peter2016arrakis}. Sockets within a same server still need to go through the network stack.

Performance requirements for intra-server IPC, inter-server RDMA and inter-server TCP/IP:

\begin{enumerate}
\item Low latency 
\item Low CPU overhead 
\item Scalable to multiple cores 
\item Scalable to many connections 
\end{enumerate}

\subsection{Goals and Existing Approaches}
\label{subsec:related}


\begin{table*}[t]
	\centering
\scalebox{0.9}{
	\begin{tabular}{l|ccc|ccc|ccc|}
		\hline
			& \multicolumn{3}{c|}{Data-plane OS} & \multicolumn{3}{c|}{Socket-like API} & \multicolumn{3}{c|}{Socket Compatible} \\
		\hline
		 	& Arrakis & IX & FreeFlow & MegaPipe & netmap & StackMap & FastSocket & libvma & SocksDirect \\
		\hline
		\hline
		Kernel Bypass & \yes &  & \yes & & \yes & \yes & & \yes & \yes \\
		\hline
		Zero Copy & \yes & \yes & \yes & \yes & & & & & \yes \\
		\hline
		Low Latency & \yes & \yes & \yes & \yes & \yes & \yes & & \yes & \yes \\
		\hline
		\hline
		Multi-core Scalability & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & \yes \\
		\hline
		Connection Scalability & & & & & & & \yes &  & \yes \\
		\hline
		\hline
		NIC-bypass IPC & & & \yes & & & & \yes & & \yes \\
		\hline
		RDMA as Transport & & & \yes & & & & & \yes & \yes \\
		\hline
		\hline
		Process Isolation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes \\
		\hline
	\end{tabular}
}
	\caption{Comparison of alternative socket implementations.}
	\label{tab:related-work}
\end{table*}


Kernel socket optimization (IX, FastSocket) 

\begin{itemize}
\item
Violate goal 1: inter-core interrupts for IPC 
\item
Violate goal 2: context switch overhead, +latency for syscall batching, still need memory copy 
\item 
Violate goal 4: Large memory footprint for many connections 
\end{itemize}

User-space stack 

\begin{itemize}
\item New abstraction (RDMA, lwip + DPDK etc.) 
\begin{itemize}
	\item Some works violate goal 3: data transmission via centralized virtual switch 
\end{itemize}
\item Compatible with socket (libvma, LOS etc.) 
\begin{itemize}
	\item Violate goal 2: memory copy 
	\item Violate goal 3: thread synchronization for multi-thread applications 
\end{itemize}
\item Common problems: 
\begin{itemize}
	\item Designed for networking, does not support or optimize for IPC communication inside the same server 
	\item Violate goal 4: Not optimized for many connections 
\end{itemize}
\end{itemize}




\subsection{Challenges for Compatibility}
\label{subsec:challenges}

Previous sections discussed general challenges for fast and scalable sockets. The semantics of Linux socket introduces a few more challenges.

%\textbf{Scalable user-space stack with process isolation.}
%To ensure isolation among processes, one category of dataplane operating systems~\cite{belay2014ix,tsai2017lite} use kernel-based network stacks. However, non-batched system calls have context switch overhead, while batched system calls increase latency. Another line of work~\cite{martins2014clickos,roghanchi2017ffwd,huang2017high} delegate socket processing to a virtual switch running on dedicated cores. The throughput of virtual switch becomes a bottleneck. Our goal is to achieve kernel bypass with process isolation, while not delegating every operation to a centralized coordinator.

\textbf{Avoid synchronization for sockets shared by multiple processes.}
Scalability analysis of Linux system calls~\cite{boyd2010analysis} suggests that many socket operations are not commutable. To be specific, a socket connection may be shared by multiple processes and threads. Threads in a same process share all socket connections. When a process forks, the parent and child processes share the sockets created before fork. The processes and threads sharing a socket forms a multi-producer and multi-consumer model. Logically, senders enqueue data into a shared FIFO, and receivers dequeue an application-specified number of bytes from the FIFO. Linux protects concurrent operations with mutual exclusion. To this end, we need a scalable socket architecture to eliminate mutual exclusion in most cases.


\textbf{Zero copy for large data buffers.}
Historically, \texttt{send} was designed as a blocking operation, so the send buffer may be overwritten by the application after \texttt{send} operation.
Non-blocking \texttt{send} preserves this semantics, so the network stack needs to copy data to an internal buffer, then send the internal buffer in background after \texttt{send} returns.
For the \texttt{recv} operation, application provides a buffer and read the data after \texttt{recv} returns.
If we implement \texttt{send} as a lazy operation, \textit{i.e.}, the data is buffered on the sender, the receiver needs to wait for a round-trip time on \texttt{recv}.
For this reason, most socket implementations send data to the receiver eagerly. Because the receiver's network stack cannot predict the user-provided \texttt{recv} address, it needs to buffer received data internally, then copy the data when \texttt{recv} is called.
Our goal is to achieve zero copy in most large data transfers without change to applications.

\textbf{Use RDMA efficiently for inter-server socket.}
The event polling API in Linux is \textit{reactive}, in the sense that the applications knows a socket connection is ready for \texttt{send} or \texttt{recv}, then issues the operation.
On the contrary, RDMA API is \textit{proactive}. The application delegates RDMA operations to the NIC hardware, and receives notification when they are completed.
Translating socket API to RDMA requires bridging the reactive and proactive semantics.
First, RDMA achieves zero copy by leaving buffer management to the application. SDP~\cite{socketsdirect} and rsockets~\cite{rsockets} are not compatible with Linux socket because the application still needs to manage send and receive buffers.
Second, two-sided RDMA applications needs to predict the size and number of messages it expects to receive, and delegate enough \texttt{recv} operations to the NIC~\cite{huang2017high}.
Third, a large number of concurrent RDMA connections would cause frequent NIC cache misses and degrade performance~\cite{mprdma,kaminsky2016design}.
Our goal is to use RDMA efficiently for existing socket applications.
