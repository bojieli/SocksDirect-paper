\section{Background}
\label{sec:background}

\subsection{Socket}
\label{subsec:motivation}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Fraction of CPU time in the kernel (socket connection setup, socket data transmission and non-socket system calls) and user-mode applications.}
%	\label{fig:socket-kernel-time}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Performance of back-end systems using inter-server socket, intra-server socket, inter-server RDMA and intra-server shared memory.}
%	\label{fig:backend-performance}
%\end{figure}


%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Latency comparison of mutual exclusion mechanisms (CAS, mutex, futex) and cache migration.}
%	\label{fig:mutual-exclusion}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Throughput (bar) and latency (line) for 16B messages using inter-server TCP socket, inter-server RDMA, intra-server TCP socket, UNIX socket, pipe and shared memory queue.}
%	\label{fig:socket-comparison}
%\end{figure}

\iffalse
\begin{table*}[t!]
	\centering
	\begin{tabular}{ll|ll|ll|ll}
		\hline
		\multicolumn{2}{c|}{Initialization} &
		\multicolumn{2}{c|}{Connection Mgmt} &
		\multicolumn{2}{c|}{Data Transmission} &
		\multicolumn{2}{c}{Process Mgmt} \\
		\hline
		API & Cat. &
		API & Cat. &
		API & Cat. &
		API & Cat. \\
		\hline
		\hline
		\textbf{socket} & Local &
		\textbf{connect} & NoPart &
		\textbf{send(to,(m)msg)} & P2P &
		\textit{(v)fork} & NoPart \\
		\hline
		bind & NoPart &
		\textbf{accept(4)} & P2P &
		\textbf{recv(from,(m)msg)} & P2P &
		\textit{pthread\_create} & NoPart \\
		\hline
		listen & NoPart &
		\textbf{\textit{fcntl, ioctl}} & Local &
		\textbf{\textit{write(v)}} & P2P &
		\textit{clone} & NoPart \\
		\hline
		socketpair & Local &
		\textbf{(get,set)sockopt} & Local &
		\textbf{\textit{read(v)}} & P2P &
		\textit{execve} & NoPart \\
		\hline
		getsockname  & Local &
		\textbf{\textit{close}, shutdown} & P2P &
		\textbf{\textit{memcpy}} & Local &
		\textit{exit} & P2P \\
		\hline
		\textbf{\textit{malloc}} & Local &
		getpeername & Local &
		\textit{(p)select} & P2P &
		\textit{sleep} & P2P \\
		\hline
		\textbf{\textit{realloc}} & Local &
		\textit{dup(2)} & P2P &
		\textit{(p)poll} & P2P &
		\textit{daemon} & P2P \\
		\hline
		\textit{epoll\_create} & Local &
		\textbf{\textit{epoll\_ctl}} & Local &
		\textbf{\textit{epoll\_(p)wait}} & P2P &
		\textit{sigaction} & Local \\
		\hline
	\end{tabular}
	\caption{Linux APIs that are related to socket and intercepted by \libipc{}. Categories include local, peer-to-peer (P2P) and non-partitionable (NoPart). APIs in \textit{italic} indicate usages besides socket. APIs in \textbf{bold} are called relatively more frequently.}
	\vspace{-10pt}
	\label{tab:socket-api}
\end{table*}
\fi

\iffalse
\begin{figure}
\begin{lstlisting}[style=CStyle]
int lfd = socket(...); // listen file descriptor (fd)
bind(lfd, listen_addr_and_port, ...);
listen(lfd, BACKLOG);
fcntl(lfd, F_SETFL, fcntl(lfd,F_GETFL,0) | O_NONBLOCK);
int efd = epoll_create(MAXEVENTS); // event fd
epoll_ctl(efd, EPOLL_CTL_ADD, lfd, ...);
while (true) { // main event loop
	int n = epoll_wait(efd, events, MAXEVENTS, 0);
	for (int i=0; i<n; i++) { // iterate events
		if (events[i].data.fd == lfd) { // new connection
			int cfd = accept(sfd, ...); // connection fd
			epoll_ctl(efd, EPOLL_CTL_ADD, cfd, ...);
			fcntl(cfd,F_SETFL,fcntl(cfd,F_GETFL,0)|O_NONBLOCK);
		}
		else if (events[i].events & EPOLLIN){//ready to recv
			do { // fetch all received data
				cnt = recv(events[i].data.fd, recvbuf, buflen);
				recvbuf = next_recv_buf();
			} while (cnt > 0);
			// do processing
		}
		else if (events[i].events & EPOLLOUT){//ready to send
			do { // flush send buf
				cnt = send(events[i].data.fd, sendbuf, sendlen);
				sendbuf += cnt; sendlen -= cnt;
			} while (cnt > 0 && sendlen > 0);
		}
	}
}
\end{lstlisting}
\vspace{-15pt}
\caption{Pseudo-code of a typical socket server application, showing most important socket operations. A socket connection is a FIFO byte-stream channel identified by an integer \emph{file descriptor} (FD). Linux enables a readiness-driven I/O multiplexing model, in which the OS tells application which FDs are ready to receive or send, then the application may prepare buffers and issue socket operations.}
\label{fig:socket-pseudo-code}
\vspace{-10pt}
\end{figure}
\fi


\begin{table*}[t]
	\centering
	\scalebox{0.68}{
		\begin{tabularx}{1.45\textwidth}{l|X|X|X|X|X|X|X|X|X|X|}
			\hline
			& FastSocket & MegaPipe / StackMap & IX & Arrakis & SandStorm / mTCP & LibVMA & OpenOnload & Rsocket / SDP & FreeFlow & SocksDirect \\
			\hline
			Reliable transport location & \multicolumn{2}{c|}{Kernel} & \multicolumn{5}{c|}{User space} & \multicolumn{3}{c|}{NIC hardware} \\
			\hline
			Wire protocol & \multicolumn{7}{c|}{TCP/IP} & \multicolumn{3}{c|}{RDMA} \\
			\hline
			Changes needed for deployment & New kernel & New kernel & New kernel & New kernel & Lib+driver & Lib+driver & Lib+driver & Lib+driver & Lib+driver +daemon & Lib+driver +daemon \\
			\hline
			\hline
			\multicolumn{11}{c|}{Compatibility} \\
			\hline
			\hline
			Transparent to existing applications & \yes &   & \yes & \yes &   & \yes & \yes & \yes & \yes & \yes \\
			\hline
			Intra-host communication & \yes & \yes & & \yes & & & & & \yes & \yes \\
			\hline
			Container overlay network & & & & & & & & & \yes & \yes \\
			\hline
			Multiple applications listen a port & \yes & \yes & & & & & & \yes & \yes & \yes \\
			\hline
			Full fork support & \yes & \yes & & & & & \yes & & & \yes \\
			\hline
			Compatible with regular TCP peers &	\yes & \yes & \yes & \yes & \yes & \yes & \yes & & & \yes \\
			\hline
			\hline
			\multicolumn{11}{c|}{Security and Isolation} \\
			\hline
			\hline
			Access control policy & Kernel & & Kernel & Kernel & & & & Kernel & Daemon & Daemon \\
			\hline
			Isolation among containers / VMs & \yes & & \yes & \yes & & & & \yes & \yes & \yes \\
			\hline
			QoS (performance isolation) & Kernel & Kernel & Kernel & NIC & NIC & NIC & NIC & NIC & Daemon & NIC \\
			\hline
			\hline
			\multicolumn{11}{c|}{Performance} \\
			\hline
			\hline
			Offload transport to RDMA NIC & & & & & & & & \yes & \yes & \yes \\
			\hline
			Many concurrent connections & \yes & \yes & \yes & \yes & \yes & & & & & \yes \\
			\hline
			Kernel bypass & & & & \yes & \yes & \yes & \yes & Partial & \yes & \yes \\
			\hline
			Scale to multiple cores & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & \yes \\
			\hline
			Multiple threads share a connection & & & & & & & & & & \yes \\
			\hline
			Multiple threads share a CPU core & & & & & & & & & & \yes \\
			\hline
			Socket-compatible zero copy & & & & & & & & & & \yes \\
			\hline
		\end{tabularx}
	}
	\vspace{-5pt}
	\caption{Comparison of high performance socket systems.}
	\label{tab:related-work}
	\vspace{-15pt}
\end{table*}


\begin{table}[t]
	\centering
	\scalebox{0.8}{
		\begin{tabular}{l|l|l|}
			\hline
			Operation	& Latency  & Throughput  \\
			& ($\mu$s) & (M~op/s) \\
			\hline
			\hline
			Inter-core cache migration	& 0.03 & 50 \\
			\hline
			System call (before KPTI) & 0.05 & 21 \\
			\hline
			CPU L3 cache miss & 0.07 & 14 \\
			\hline
			Atomic operation & 0.10 & 5$\sim$10 \\
			\hline
			Lockless shared memory queue & 0.25 & 27 \\
			\hline
			\textbf{Intra-host \sys} & 0.30 & 22 \\
			\hline
			System call (after KPTI) & 0.20 & 5.0 \\
			\hline
			Copy one page (4~KiB) & 0.40 & 5.0 \\
			\hline
			NIC cache miss & 0.45 & 2.2 \\
			\hline
			Cooperative context switch & 0.52 & 2.0 \\
			\hline
			Map 1 page (4~KiB) & 0.78 & 1.3 \\
			\hline
			CPU to NIC hairpin RDMA & 1.0 & 14 \\
			\hline
			Atomic shared memory queue & 1.0 & 6.1 \\
			\hline
			Map 32 pages (128~KiB) & 1.2 & 0.8 \\
			\hline
			Two-sided inter-host RDMA & 1.6 & 8 \\
			\hline
			One-sided inter-host RDMA & 1.6 & 13 \\
			\hline
			\textbf{\sys via RDMA} & 1.6 & 8 \\
			\hline
			Semaphore, mutex, futex & 2.8$\sim$5.5 & 0.2$\sim$0.4 \\
			\hline
			Intra-host Linux TCP & 11 & 0.9 \\
			\hline
			Copy 32 pages (128~KiB) & 13 & 0.08 \\
			\hline
			Inter-host Linux TCP & 30 & 0.3 \\
			\hline
		\end{tabular}
	}
	\vspace{-5pt}
	\caption{Round-trip latency and per-core throughput of operations (testbed settings in Sec.\ref{subsec:methodology}). Message size is 8 bytes if not specified.}
	\label{tab:operation-performance}
	\vspace{-15pt}
\end{table}


Socket is the standard communication primitive among applications and containers.
%For example, a web service typically contains an HTTP load balancer, Web application, key-value store and database, which communicate via TCP or UNIX socket.
%Socket is also used to exchange data between parameter server and workers in Tensorflow and from mapper to reducer in Spark.
%The socket API was designed in 1980s when network communication has millisecond scale latency, so the computation overhead in OS networking stack is negligible.
%In contrast, modern data centers have networks with microsecond scale latency and throughput comparable to intra-host buses, making socket a well-known bottleneck for communication intensive applications.
Modern data center networks have microsecond-level base latency and tens of Gbps throughput, making socket a well-known bottleneck for communication intensive applications running on multiple hosts. In addition to inter-host communications, cloud applications and containers at the same host often communicate with each other, making intra-host socket communication increasingly important in the cloud era. Under stress tests, we observe applications such as Nginx~\cite{reese2008nginx}, memcached~\cite{fitzpatrick2004distributed} and Redis~\cite{carlson2013redis} consume 50\% to 90\% CPU time in the kernel, mostly dealing with TCP socket operations.

%To understand the cost of socket, we first need to know the functions of socket.
TCP and Unix socket in a modern OS typically have three functions:
\begin{ecompact}
	\item Address, locate and connect to another application; % in the system via IP and port;
	\item Provide a reliable and ordered communication channel, identified by an integer \emph{file descriptor} (FD);
	\item Multiplex events from multiple channelsï¼Œ e.g., poll and epoll. Most Linux applications use a readiness-driven I/O multiplexing model. The OS tells application which FDs are ready to receive or send, then the application may prepare buffers and issue receive or send operations.
\end{ecompact}

Traditional Linux socket is implemented in the OS kernel space with shared data structures. It is not optimized for high performance due to expensive kernel context switching and data structure synchronization costs.
%Most modern applications use socket in an event-driven style. similar to Figure~\ref{fig:socket-pseudo-code}.

%Socket is a well-known bottleneck for communication intensive applications. We stress test Nginx load balancer, memcached and Redis with one request per TCP connection or a stream of requests on pre-established TCP connections, and find that 50\%$\sim$90\% CPU time is consumed in the kernel, mostly dealing with socket system calls. The one request per connection scenario shows much lower throughput, because of overhead in connection creation.

%If we were able to mitigate the overhead associated with sockets, application performance would be 2x$\sim$10x.
%As another example, we replaced socket with RDMA and enabled zero copy gRPC in distributed Tensorflow~\cite{abadi2016tensorflow}, and training a VGG network shows 5x speedup.

%Socket is a well-known bottleneck in fast data center networks \RED{(cite)}. In a data center, we categorize systems that process queries from Internet users to be \textit{front-end} (\textit{e.g.} DNS server, load balancer and Web service), and the other systems (\textit{e.g.} database, stream processing and machine learning) to be \textit{back-end}.

%For front-end systems, as shown in Figure~\ref{fig:socket-kernel-time}, 70\% -- 90\% CPU time is used in socket system calls. They maintain a large number of socket connections to Internet users. Establishing a socket connection takes $\approx$40$\mu$s CPU time, so a CPU core can only serve 25K new connections per second~\cite{lin2016scalable}.

%For back-end systems, the performance using socket is significantly worse than using RDMA or shared memory. This is because back-end systems interact frequently with other nodes and services, consequently the latency is vital for their performance. As shown in Figure~\ref{fig:socket-comparison}, socket latency between different processes in a same server is $\approx$10~$\mu$s, even much higher than inter-server RDMA latency ($\approx$2~$\mu$s).

%\textbf{Event-driven programming.}

%\textbf{Socket process: connection setup and data transmission.}

\subsection{High Performance Socket Systems}
\label{subsec:related-work}


Many high performance socket systems have been proposed from both academia and industry. Table~\ref{tab:related-work} compares several of them from three aspects: compatibility, security and performance.

The first line of work optimizes the kernel TCP/IP stack. FastSocket~\cite{lin2016scalable}, Affinity-Accept~\cite{pesterev2012improving}, FlexSC~\cite{soares2010flexsc} and zero-copy socket~\cite{thadani1995efficient,chu1996zero,linux-zero-copy} achieve good compatibility and security, but leave some performance optimizations on the table.
MegaPipe~\cite{han2012megapipe} and StackMap~\cite{yasukata2016stackmap} propose new interfaces to achieve zero copy and improve I/O multiplexing, at the cost of requiring application modifications.

The second line of work completely bypasses kernel TCP/IP stack and implements TCP/IP in user space.
In this category, IX~\cite{belay2017ix} and Arrakis~\cite{peter2016arrakis} are new OS architectures that uses virtualization to ensure security and isolation.
IX leverages user space TCP/IP stack~\cite{dunkels2001design} while using kernel to forward every packet for performance isolation and QoS.
In contrast, Arrakis offloads QoS to NIC, therefore bypasses kernel for data plane.
Apart from these new OS architectures, many recent efforts use high performance packet I/O frameworks on Linux, e.g., Netmap~\cite{rizzo2012netmap}, Intel DPDK~\cite{dpdk} and PF\_RING~\cite{pf-ring}), to directly access NIC queues in user space.
SandStorm~\cite{marinos2014network}, mTCP~\cite{jeong2014mtcp}, Seastar~\cite{seastar} and F-Stack~\cite{fstack} propose new interfaces and thus need to modify applications. LibVMA~\cite{libvma}, OpenOnload~\cite{openonload}, DBL~\cite{dbl} and LOS~\cite{huang2017high} are designed to be compatible with existing applications.
These designs often sacrifice security, because multiple applications of a packet I/O framework share a NIC. To receive packets of listening ports and established connections from the NIC directly, each application inserts flow steering rules to the NIC independently, which is insecure and leads to conflicts when multiple applications listen on the same port. An application may also send arbitrary packets that violate access control policy. Moreover, most user-space stacks are not designed to accelerate intra-host connections and have limited fork support.
\RED{Wei: The above paragraph looks very confusing.}

The third line of research utilize RDMA NICs~\cite{mellanox} that are widely available in modern data centers~\cite{guo2016rdma} and translate socket operations to RDMA verbs. RDMA uses hardware offloading to provide ultra low latency and near zero CPU utilization compared to software-based TCP/IP network stacks.
RSocket~\cite{rsockets} and SDP~\cite{socketsdirect} converts each socket connection to an RDMA connection.
RDMA NIC needs to keep track of per-connection states using a $\approx$2~MiB~\cite{kalia2018datacenter} on-NIC memory as cache. With hundreds of concurrent connections, we will suffer from frequent cache misses, resulting in serious throughput degradation~\cite{mprdma,kaminsky2016design}. FreeFlow~\cite{freeflow, nsdi19freeflow} is a software switch to virtualize an RDMA NIC for container overlay network and enables intra-host RDMA communication.
It uses RSocket to translate socket to RDMA, thus also suffer from cache miss with many connections.
The software switch also adds CPU and latency overheads.

\subsection{Performance Challenges}
\label{subsec:performance-challenges}

We aim to deliver consistent high throughput and low latency regardless of the number of CPU cores, concurrent connections and threads.
This subsection analyzes challenges to achieve this performance goal.

\parab{Intra-host communication.}
Most existing approaches to intra-host socket either use kernel TCP stack or NIC loopback.
The kernel TCP stack has evolved to become quite complicated over the years~\cite{yasukata2016stackmap}, which is an overkill 
for intra-server communication. % may not need many TCP features, e.g., congestion control and loss recovery.
Arrakis uses the NIC to forward packets from one application to another.
As shown in Table~\ref{tab:operation-performance}, the hairpin latency from CPU to NIC is 25x higher than inter-core cache migration delay.
The throughput is also limited by MMIO doorbell latency and PCIe bandwidth~\cite{neugebauer2018understanding,li2017kv}.

We aim to leverage user-space shared memory for intra-host transport.

\parab{Container overlay network.}
Many container deployments use isolated network namespaces for containers, which communicate via a virtual overlay network.
In Linux, a virtual switch daemon~\cite{pfaff2015design} forwards packets among host NIC and virtual NICs in containers.
This architecture incurs the overhead of multiple context switches and memory copies on each packet, and the virtual switch is a scalability bottleneck~\cite{pfefferle2015hybrid}.

We aim to use the virtual switch for control plane only and implement data plane operations peer-to-peer.

\parab{Offloading transport to RDMA NIC.}
%For inter-host communication inside data centers, there are two challenges for using RDMA.
%The first is to transparently determine whether or not the remote host supports \sys{}.
%We cannot use an out-of-band communication channel such as RDMA CM because it cannot pass through middleboxes in cloud network.
%The second is to bridge the semantics of socket and RDMA~\cite{dragojevic2014farm}.
The main challenge for leverage RDMA for inter-host socket communication is to bridge the semantics of socket and RDMA~\cite{dragojevic2014farm}.
For example, RDMA preserves messages boundaries while TCP does not.
For I/O multiplexing, RDMA provides a completion notification model while event polling in Linux socket requires a readiness model~\cite{han2012megapipe}.
Further, one-sided and two-sided RDMA verbs have different efficiency and overheads~\cite{kalia2014using,kaminsky2016design}.

We aim to use RDMA efficiently for socket applications, while fall back to TCP transparently for non-RDMA peers.

\parab{Many concurrent connections.}
Internet facing applications often need to serve millions of concurrent connections efficiently~\cite{nishtala2013scaling,lin2016scalable,belay2017ix}.
%In addition, because a socket connection provides a FIFO abstraction and the OS offers event multiplexing, 
Moreover, it is also common for two processes to create large number of connections between them, where each connection handles a concurrent task.
In Linux, a socket connection has dedicated send and receive buffers, each is at least one page (4~KiB) in size~\cite{davidskbs}. With millions of concurrent connections, the socket buffers can consume gigabytes of memory, most of which is empty.
Random access to a large number of buffers also leads to CPU cache and TLB miss.
Similar issue exists in RDMA NICs with limited on-chip memory for maintaining connection states.
%, which is slower than sequential access to a single buffer~\cite{li2017kv}. 
%This problem is exaggerated in RDMA. RDMA NIC caches connection states in limited on-board memory. Just over a few hundred active RDMA connections could cause cache misses and degrade performance~\cite{mprdma,kaminsky2016design}. 
%Moreover, event-driven applications use \texttt{epoll} to detect which connections are ready for send or receive. If the event queue is separated from data queues, each \texttt{recv} operation involves two cache migrations for event and data~\cite{yasukata2016stackmap}. 

We aim to minimize memory cache miss per data transmission by multiplex socket connections.


\parab{Kernel bypassing.}
Traditionally, socket APIs are implemented in kernel and thus require kernel crossing for each socket operation.
To make it worse, the Kernel Page-Table Isolation patches~\cite{kpti} to protect against Meltdown~\cite{Lipp2018meltdown} attacks make kernel crossings 4x expensive.
% (from 47~ns on Linux 4.11 to 200~ns on Linux 4.16 per system call). 
%Together with additional cache invalidations, KPTI patches reduce TCP socket throughput by 40\% for 64-byte messages. %These side-channel attacks also make us rethink the Linux security model in which the complicated network stack needs to be trusted and protected. %, while sharing cores with applications.
%To ensure isolation among processes, one category of dataplane operating systems~\cite{belay2017ix,tsai2017lite} use kernel-based network stacks. However, non-batched system calls have context switch overhead, while batched system calls increase latency.

We aim to bypass kernel without compromising security.


\parab{Scaling to multiple cores.}
Linux kernel acquires several global locks during connection creation~\cite{boyd2008corey,boyd2010analysis}, while MegaPipe~\cite{han2012megapipe} and FastSocket~\cite{lin2016scalable} remove these bottlenecks.
Several works~\cite{martins2014clickos,roghanchi2017ffwd,huang2017high,freeflow} delegate all operations to a virtual switch running as a daemon process, which is a scalability bottleneck in modern servers with tens of cores.

We aim to minimize centralized coordination while preserving isolation.

\parab{Multiple threads sharing a connection.}
%Many applications are multi-threaded for two reasons.
%First, unlike FreeBSD, the asynchronous interface for reading and writing disk files in Linux cannot leverage OS cache and buffer, so the applications keep using a synchronous interface with multiple threads~\cite{nginx-multi-thread}.
%Second, many web application frameworks prefer a synchronous programming model to process each user request because it is easier to write and debug~\cite{barroso2017attack}.
Multiple threads in a process share socket connections.
For example, after a process forks, both parent and child processes share existing sockets.
Sockets can also be passed to another process through Unix domain socket.
To protect concurrent operations, Linux kernel acquires a per-socket lock for each socket operation~\cite{boyd2010analysis,han2012megapipe,lin2016scalable}.
Table~\ref{tab:operation-performance} shows that a shared memory queue protected by atomic operations has 4x latency and 22\% throughput of a lockless queue, even if there is no contention.
Previous works~\cite{boyd2010analysis,clements2015scalable} suggest that many socket operations are not commutable and thus synchronization cannot always be avoided.

We aim to minimize synchronization overhead in two ways.
First, we optimize for the common case and remove synchronization in frequent socket operations.
Second, we leverage the fact that shared memory message passing is much cheaper than locking~\cite{roghanchi2017ffwd}, and use message passing as the exclusive synchronization mechanism.


\parab{Multiple threads sharing a CPU core.}
Most user-space socket stacks use polling to avoid the overhead of interrupt and thread wakeup in the kernel.
However, polling does not work when multiple threads have to share a core.
To switch thread contexts, as Table~\ref{tab:operation-performance} shows, using semaphore, mutex or \texttt{futex} to wake up a sleeping thread is 5x$\sim$10x slower than cooperative context switch via \texttt{sched\_yield}.
%This extra latency is due to event notification mechanism and process scheduler in kernel.

We aim to use cooperative multitasking instead of kernel thread wakeup mechanisms.


\parab{Zero copy.}
The semantics \texttt{send} and \texttt{recv} cause memory copies between application and network stack. For non-blocking \texttt{send}, the system needs to copy data out of the buffer because the application may overwrite the buffer right after \texttt{send} returns. %Simply removing these copies may violate application correctness. 
For \texttt{recv}, application provides a buffer as the data destination. %If we buffer data on the sender and only transmit it on \texttt{recv}, the receiver needs to wait for a round-trip time. For this reason, socket implementations buffer data on the receiver and copy it to application on \texttt{recv}.
System needs to copy data received into the buffer.
Many user-space sockets provide both standard socket API and an alternative zero-copy API, but none of them achieves zero copy for the standard API on both send and receive paths.

We aim to allow zero copy for large transfers. Rather than copying, we remap the physical pages to new virtual addresses. As Table~\ref{tab:operation-performance} shows, remapping a single page is more expensive than copying it because of kernel crossing and TLB flush costs. Hence, we batch page mapping operations to amortize the overhead.



%Spinlock in kernel is implemented with Compare-And-Swap (CAS) instruction, which costs $100\sim200$~ns per acquire and release. In comparison, shared memory message passing only needs one cache migration~\cite{roghanchi2017ffwd} (30~ns).
%In comparison, shared memory message passing
%Scalability analysis of Linux system calls~\cite{boyd2010analysis,clements2015scalable} suggests that many socket operations are not commutable and thus impossible to scale for all cases. 
%For example, socket provides \texttt{SO\_REUSEPORT} option. When this option is enabled, multiple processes on one host can \texttt{bind} to the same port and incoming connections are dispatched to the listeners. During connection setup, coordination is required to allocate file descriptors and port numbers, load balance connections and allocate buffers. In addition, a socket may be shared by multiple processes and threads in a same process share socket connections. When a process forks, the parent and child processes also share the sockets created before fork.


%For inter-server data transmission, Linux kernel copies data four times: on the send side, from application to kernel socket buffer, then to NIC ring buffer; the receive side is similar. For intra-server, data is copied three times: from application to kernel send buffer, then to kernel receive buffer via loopback interface, and finally to another application. Each CPU core can copy $\approx$5~GiB data per second~\cite{panda2016netbricks}, that's why a kernel TCP connection is hard to saturate an 100~Gbps NIC.
%Historically, \texttt{send} was designed as a blocking operation, and the send buffer may be overwritten by the application after \texttt{send} function call.
%In a non-blocking \texttt{send}, the process could overwrite the send buffer after \texttt{send} function call returns. To avoid race condition on the data buffer, network stack needs to copy data to an internal buffer. %before \texttt{send} returns, and then send the copied data in background.
%For the \texttt{recv} operation, application provides a buffer and read the data after \texttt{recv}.
%If we implement \texttt{send} as a lazy operation, \textit{i.e.}, the data is buffered on the sender, the receiver needs to wait for a round-trip time on \texttt{recv}.
%For this reason, most socket implementations send data to the receiver eagerly.
%Because the receiver's network stack cannot predict the user-provided \texttt{recv} address, it needs to buffer received data internally, then copy to the desired destination when \texttt{recv} is called.
%Our goal in \sys is to achieve zero copy for large data transfers without making changes to applications. We accomplish this by taking advantage of the page mapping mechanism in virtual memory. 







%\textbf{Virtual file system.}
%Each socket connection is a \textit{file descriptor} (FD) in Linux. First, each new connection involves expensive \texttt{inode} and \texttt{dentry} creation in the \texttt{proc} file system, which is not scalable and rarely used except for diagnostic and monitoring utilities. Second, socket operations need to pass through the virtual file system (VFS), which introduces CPU overhead and additional latency.





%\subsection{High Performance Socket Systems}
%\label{sec:related}


\iffalse
\begin{table*}[t]
	\centering
	\scalebox{0.88}{
		\begin{tabular}{l|c|ccc|cc|ccc|}
			\hline
			Category	& \multicolumn{1}{c|}{Linux Opt.} & \multicolumn{3}{c|}{New Kernel Stack} & \multicolumn{2}{c|}{User-space Packet} & \multicolumn{3}{c|}{User-space Socket} \\
			\hline
			System	& FastSocket & IX & MegaPipe & StackMap & Arrakis & FreeFlow & mTCP & libvma & SocksDirect \\
			\hline
			\hline
			Socket-like API & \yes & & \yes & \yes & \yes & & \yes & \yes & \yes \\
			\hline
			Linux Compatible & \yes & & & & & & & \yes & \yes \\
			\hline
			Process Isolation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes \\
			\hline
			\hline
			Kernel Bypass & & & & & \yes & \yes & \yes & \yes & \yes \\
			\hline
			Cooperative Multitasking & & & & & & & & & \yes \\
			\hline
			Zero Copy & & \yes & & \yes & \yes & \yes & & & \yes \\
			\hline
			\hline
			NIC-bypass IPC & \yes & & \yes & \yes & & \yes & \yes & & \yes \\
			\hline
			RDMA as Transport & & & & & & \yes & & & \yes \\
			\hline
			\hline
			Scalable Socket Creation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & \yes \\
			\hline
			Lock-free Multi-thread & & & & & & & & & \yes \\
			\hline
			Scale to Many Sockets & \yes & \yes & \yes & \yes & & & \yes & & \yes \\
			\hline
		\end{tabular}
	}
	\caption{Comparison of high performance socket systems.}
	\label{tab:old-related-work}
	\vspace{-10pt}
\end{table*}
\fi
