\section{Background}
\label{sec:background}

\subsection{Socket: the Performance Bottleneck}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Fraction of CPU time in the kernel (socket connection setup, socket data transmission and non-socket system calls) and user-mode applications.}
	\label{fig:socket-kernel-time}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Performance of back-end systems using inter-server socket, intra-server socket, inter-server RDMA and intra-server shared memory.}
	\label{fig:backend-performance}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Throughput (bar) and latency (line) for 16B messages using inter-server TCP socket, inter-server RDMA, intra-server TCP socket, UNIX socket, pipe and shared memory queue.}
	\label{fig:socket-comparison}
\end{figure}


Socket is a well-known bottleneck in fast data center networks \RED{(cite)}. In a data center, we categorize systems that process queries from Internet users to be \textit{front-end} (\textit{e.g.} DNS server, load balancer and Web service), and the other systems (\textit{e.g.} database, stream processing and machine learning) to be \textit{back-end}.

For front-end systems, as shown in Figure~\ref{fig:socket-kernel-time}, 70\% -- 90\% CPU time is used in socket system calls. They maintain a large number of socket connections to Internet users. Establishing a socket connection takes $\approx$40$\mu$s CPU time, so a CPU core can only serve 25K new connections per second~\cite{lin2016scalable}.

For back-end systems, as shown in Figure~\ref{fig:backend-performance}, the performance using socket is significantly worse than using RDMA or shared memory. This is because back-end systems interact frequently with other nodes and services, consequently the latency is vital for their performance. As shown in Figure~\ref{fig:socket-comparison}, socket latency between different processes in a same server is $\approx$10~$\mu$s, even much higher than inter-server RDMA latency ($\approx$2~$\mu$s).

\textbf{Socket process: connection setup and data transmission.}

There has been much work~\cite{peter2016arrakis,lin2016scalable} to analyze the source of socket processing overhead.

\textbf{Context switch.}
Socket APIs are implemented in kernel space and thus require context switch for each socket operation. To protect against side-channel attacks based on speculative execution~\cite{Kocher2018spectre} and caching~\cite{Lipp2018meltdown}, system calls become more expensive~\cite{kpti}.

\textbf{Network stack.}
In Linux socket, inter-server connections need to go through the TCP/IP stack, and receiving each packet takes $\approx$3$\mu$s CPU time~\cite{peter2016arrakis}. In addition, processes communicating within a same server still need to communicate through the network stack.

\textbf{Data copy.}
For inter-server data transmission, Linux kernel copies data four times: on the send side, from the application buffer to kernel socket buffer, then to NIC ring buffer; the receive side is similar.
For intra-server transmission, data is copied three times: from the send application to kernel send buffer, then to kernel receive buffer and finally to the receive application.
%However, each CPU core can only copy 5~GiB data per second, limiting throughput for applications that operate on large chunks of data~\cite{panda2016netbricks}.

\textbf{Synchronization.}
Scalability analysis of Linux system calls~\cite{boyd2010analysis} suggests that many socket operations are not commutable. The Linux kernel needs to acquire global locks for socket connection setup and a per-socket lock for each socket operation. The synchronization overhead limits scalability of Linux socket.


List several related work on fast sockets. They are not compatible with existing applications. Many works propose to use user-level TCP/IP stack \RED{(cite)} or RDMA \RED{(cite)}...

The Multikernel design: replaces locks in kernel with message passing, but does not remove context switch.


\subsection{Challenges for High Performance Socket}
\label{subsec:challenges}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Latency comparison of mutual exclusion mechanisms (CAS, mutex, futex) and cache migration.}
	\label{fig:mutual-exclusion}
\end{figure}

\textbf{Scalable user-space stack with process isolation.}
To ensure isolation among processes, one category of dataplane operating systems~\cite{belay2014ix,tsai2017lite} use kernel-based network stacks. However, non-batched system calls have context switch overhead, while batched system calls increase latency. Another line of work~\cite{martins2014clickos,roghanchi2017ffwd,huang2017high} delegate socket processing to a virtual switch running on dedicated cores. The throughput of virtual switch becomes a bottleneck. Our goal is to achieve kernel bypass with process isolation, while not delegating every operation to a centralized coordinator.

%During connection setup, we need to coordinate \texttt{listen} port numbers, load balance \texttt{connect} requests and allocate resources.
%For data transmission, modern commodity NICs support multiple queues with receive flow steering~\cite{herbertrfs,mellanox}, so connection multiplexing and isolation can be offloaded to the NIC. Our goal is to achieve multi-core scalability for both connection setup and data transmission.

\textbf{Avoid synchronization for shared socket send and receive.}
A socket connection may be shared by multiple processes and threads. Threads in a same process share all socket connections. When a process forks, the parent and child processes share the sockets created before fork. The processes and threads sharing a socket forms a multi-producer and multi-consumer model. Logically, senders enqueue data into a shared FIFO, and receivers dequeue an application-specified number of bytes from the FIFO. Linux protects concurrent operations with mutual exclusion.

Ideally, inter-process message passing requires only one cache migration~\cite{roghanchi2017ffwd}. However, as shown in Figure~\ref{fig:mutual-exclusion}, the Compare-And-Swap (CAS) instruction for mutual exclusion, even in non-contended case, is significantly slower than cache migration. Kernel mutex and user-mode futex are even slower than CAS. Mutual exclusion becomes a bottleneck. In addition, both mutual exclusion and contended memory write are not scalable to multiple cores~\cite{boyd2014optimizing}. To this end, we need a scalable socket architecture to eliminate mutual exclusion in most cases.

\textbf{Minimize per-socket memory footprint.}
Each process may connect to multiple Internet hosts and data center servers. In Linux, each socket connection has dedicated send and receive buffers, and each buffer is at least one page (4~KiB). With one million concurrent connections, the socket buffers consumes tens of gigabytes of memory, most of which is empty~\cite{lin2016scalable}. Accessing many buffers randomly also leads to non-cached memory accesses, which is slower than sequential access to a single buffer~\cite{li2017kv}. Our goal is to support billions of concurrent connections in a server.

\textbf{Zero copy for large data buffers.}
Historically, \texttt{send} was designed as a blocking operation, so the send buffer may be overwritten by the application after \texttt{send} operation.
Non-blocking \texttt{send} preserves this semantics, so the network stack needs to copy data to an internal buffer, then send the internal buffer in background after \texttt{send} returns.
For the \texttt{recv} operation, application provides a buffer and read the data after \texttt{recv} returns.
If we implement \texttt{recv} as a lazy operation, \textit{i.e.}, the data is buffered on the sender, the receiver needs to wait for a round-trip time on \texttt{recv}.
Otherwise, as in most socket implementations, we send data to the receiver before \texttt{recv}. Because the receiver's network stack cannot predict the user-provided \texttt{recv} address, it needs to buffer received data internally, then copy the data when \texttt{recv} is called.
Our goal is to achieve zero copy in most large data transfers without change to applications.

\textbf{Use RDMA efficiently for inter-server socket.}
The event polling API in Linux is \textit{reactive}, in the sense that the applications knows a socket connection is ready for \texttt{send} or \texttt{recv}, then issues the operation.
On the contrary, RDMA API is \textit{proactive}. The application delegates RDMA operations to the NIC hardware, and receives notification when they are completed.
Translating socket API to RDMA requires bridging the reactive and proactive semantics.
First, RDMA achieves zero copy by leaving buffer management to the application. SDP~\cite{socketsdirect} and rsockets~\cite{rsockets} are not compatible with Linux socket because the application still needs to manage send and receive buffers.
Second, two-sided RDMA applications needs to predict the size and number of messages it expects to receive, and delegate enough \texttt{recv} operations to the NIC~\cite{huang2017high}.
Third, a large number of concurrent RDMA connections would cause frequent NIC cache misses and degrade performance~\cite{mprdma,kaminsky2016design}.
Our goal is to use RDMA efficiently for existing socket applications.
