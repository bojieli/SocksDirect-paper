\section{Background}
\label{sec:background}

\subsection{Socket}
\label{subsec:motivation}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Fraction of CPU time in the kernel (socket connection setup, socket data transmission and non-socket system calls) and user-mode applications.}
%	\label{fig:socket-kernel-time}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Performance of back-end systems using inter-server socket, intra-server socket, inter-server RDMA and intra-server shared memory.}
%	\label{fig:backend-performance}
%\end{figure}


%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Latency comparison of mutual exclusion mechanisms (CAS, mutex, futex) and cache migration.}
%	\label{fig:mutual-exclusion}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Throughput (bar) and latency (line) for 16B messages using inter-server TCP socket, inter-server RDMA, intra-server TCP socket, UNIX socket, pipe and shared memory queue.}
%	\label{fig:socket-comparison}
%\end{figure}

\iffalse
\begin{table*}[t!]
	\centering
	\begin{tabular}{ll|ll|ll|ll}
		\hline
		\multicolumn{2}{c|}{Initialization} &
		\multicolumn{2}{c|}{Connection Mgmt} &
		\multicolumn{2}{c|}{Data Transmission} &
		\multicolumn{2}{c}{Process Mgmt} \\
		\hline
		API & Cat. &
		API & Cat. &
		API & Cat. &
		API & Cat. \\
		\hline
		\hline
		\textbf{socket} & Local &
		\textbf{connect} & NoPart &
		\textbf{send(to,(m)msg)} & P2P &
		\textit{(v)fork} & NoPart \\
		\hline
		bind & NoPart &
		\textbf{accept(4)} & P2P &
		\textbf{recv(from,(m)msg)} & P2P &
		\textit{pthread\_create} & NoPart \\
		\hline
		listen & NoPart &
		\textbf{\textit{fcntl, ioctl}} & Local &
		\textbf{\textit{write(v)}} & P2P &
		\textit{clone} & NoPart \\
		\hline
		socketpair & Local &
		\textbf{(get,set)sockopt} & Local &
		\textbf{\textit{read(v)}} & P2P &
		\textit{execve} & NoPart \\
		\hline
		getsockname  & Local &
		\textbf{\textit{close}, shutdown} & P2P &
		\textbf{\textit{memcpy}} & Local &
		\textit{exit} & P2P \\
		\hline
		\textbf{\textit{malloc}} & Local &
		getpeername & Local &
		\textit{(p)select} & P2P &
		\textit{sleep} & P2P \\
		\hline
		\textbf{\textit{realloc}} & Local &
		\textit{dup(2)} & P2P &
		\textit{(p)poll} & P2P &
		\textit{daemon} & P2P \\
		\hline
		\textit{epoll\_create} & Local &
		\textbf{\textit{epoll\_ctl}} & Local &
		\textbf{\textit{epoll\_(p)wait}} & P2P &
		\textit{sigaction} & Local \\
		\hline
	\end{tabular}
	\caption{Linux APIs that are related to socket and intercepted by \libipc{}. Categories include local, peer-to-peer (P2P) and non-partitionable (NoPart). APIs in \textit{italic} indicate usages besides socket. APIs in \textbf{bold} are called relatively more frequently.}
	\vspace{-10pt}
	\label{tab:socket-api}
\end{table*}
\fi

\begin{figure}
\begin{lstlisting}[style=CStyle]
int lfd = socket(...); // listen file descriptor (fd)
bind(lfd, listen_addr_and_port, ...);
listen(lfd, BACKLOG);
fcntl(lfd, F_SETFL, fcntl(lfd,F_GETFL,0) | O_NONBLOCK);
int efd = epoll_create(MAXEVENTS); // event fd
epoll_ctl(efd, EPOLL_CTL_ADD, lfd, ...);
while (true) { // main event loop
	int n = epoll_wait(efd, events, MAXEVENTS, 0);
	for (int i=0; i<n; i++) { // iterate events
		if (events[i].data.fd == lfd) { // new connection
			int cfd = accept(sfd, ...); // connection fd
			epoll_ctl(efd, EPOLL_CTL_ADD, cfd, ...);
			fcntl(cfd,F_SETFL,fcntl(cfd,F_GETFL,0)|O_NONBLOCK);
		}
		else if (events[i].events & EPOLLIN){//ready to recv
			do { // fetch all received data
				cnt = recv(events[i].data.fd, recvbuf, buflen);
				recvbuf = next_recv_buf();
			} while (cnt > 0);
			// do processing
		}
		else if (events[i].events & EPOLLOUT){//ready to send
			do { // flush send buf
				cnt = send(events[i].data.fd, sendbuf, sendlen);
				sendbuf += cnt; sendlen -= cnt;
			} while (cnt > 0 && sendlen > 0);
		}
	}
}
\end{lstlisting}
\vspace{-15pt}
\caption{Pseudo-code of a typical socket server application, showing several most important socket operations.}
\label{fig:socket-pseudo-code}
\vspace{-10pt}
\end{figure}



\begin{table*}[t]
	\centering
	\scalebox{0.68}{
		\begin{tabularx}{1.45\textwidth}{l|X|X|X|X|X|X|X|X|X|X|}
			\hline
			& FastSocket & MegaPipe / StackMap & IX & Arrakis & SandStorm / mTCP & LibVMA & OpenOnload & Rsocket / SDP & FreeFlow & SocksDirect \\
			\hline
			\hline
			\multicolumn{11}{c|}{Compatibility} \\
			\hline
			\hline
			Transparent to existing applications & \yes &   & \yes & \yes &   & \yes & \yes & \yes & \yes & \yes \\
			\hline
			Intra-host communication & \yes & \yes & & \yes & & & & & \yes & \yes \\
			\hline
			Container overlay network & & & & & & & & & \yes & \yes \\
			\hline
			Multiple applications listen a port & \yes & \yes & & & & & & \yes & \yes & \yes \\
			\hline
			Full fork support & \yes & \yes & & & & & \yes & & & \yes \\
			\hline
			Compatible with regular TCP peers &	\yes & \yes & \yes & \yes & \yes & \yes & \yes & & & \yes \\
			\hline
			Deployment complexity & New kernel & New kernel & New kernel & New kernel & Lib+driver & Lib+driver & Lib+driver & Lib+driver & Lib+driver +daemon & Lib+driver +daemon \\
			\hline
			\hline
			\multicolumn{11}{c|}{Security and Isolation} \\
			\hline
			\hline
			Access control policy & Kernel & & Kernel & Kernel & & & & Kernel & Daemon & Daemon \\
			\hline
			Isolation among containers / VMs & \yes & & \yes & \yes & & & & \yes & \yes & \yes \\
			\hline
			QoS (performance isolation) & Kernel & Kernel & Kernel & NIC & NIC & NIC & NIC & NIC & Daemon & NIC \\
			\hline
			\hline
			\multicolumn{11}{c|}{Performance} \\
			\hline
			\hline
			Offload transport to RDMA NIC & & & & & & & & \yes & \yes & \yes \\
			\hline
			Many concurrent connections & \yes & \yes & \yes & \yes & \yes & & & & & \yes \\
			\hline
			Kernel bypass & & & & \yes & \yes & \yes & \yes & Partial & \yes & \yes \\
			\hline
			Scale to multiple cores & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & \yes \\
			\hline
			Multiple threads share a connection & & & & & & & & & & \yes \\
			\hline
			Multiple threads share a CPU core & & & & & & & & & & \yes \\
			\hline
			Socket-compatible zero copy & & & & & & & & & & \yes \\
			\hline
		\end{tabularx}
	}
	\vspace{-10pt}
	\caption{Comparison of high performance socket systems.}
	\label{tab:related-work}
	\vspace{-10pt}
\end{table*}


Socket is the standard communication primitive among applications and containers.
For example, a web service typically contains an HTTP load balancer, Web application, key-value store and database, which communicate via TCP or UNIX socket.
Socket is also used to exchange data between parameter server and workers in Tensorflow and from mapper to reducer in Spark.

The socket API was designed in 1980s when network communication has millisecond scale latency, so the computation overhead in OS networking stack is negligible.
In contrast, modern data centers have networks with microsecond scale latency and throughput comparable to intra-host buses, making socket a well-known bottleneck for communication intensive applications.
For example, under our stress tests, Nginx load balancer, memcached and Redis key-value store consume 50\% to 90\% CPU time in the kernel, mostly dealing with socket operations.
Additionally, a modern server in data center contains tens of cores, therefore multiple applications and containers run on a same host, making intra-host socket communication increasingly important.

Socket in a modern OS provides three functions:
(1) address, locate and connect to another application in the system via IP and port;
(2) a reliable and ordered communication channel;
(3) multiplex events from multiple channels.
Most modern applications use socket in an event-driven style similar to Figure~\ref{fig:socket-pseudo-code}.

%Socket is a well-known bottleneck for communication intensive applications. We stress test Nginx load balancer, memcached and Redis with one request per TCP connection or a stream of requests on pre-established TCP connections, and find that 50\%$\sim$90\% CPU time is consumed in the kernel, mostly dealing with socket system calls. The one request per connection scenario shows much lower throughput, because of overhead in connection creation.

%If we were able to mitigate the overhead associated with sockets, application performance would be 2x$\sim$10x.
%As another example, we replaced socket with RDMA and enabled zero copy gRPC in distributed Tensorflow~\cite{abadi2016tensorflow}, and training a VGG network shows 5x speedup.

%Socket is a well-known bottleneck in fast data center networks \RED{(cite)}. In a data center, we categorize systems that process queries from Internet users to be \textit{front-end} (\textit{e.g.} DNS server, load balancer and Web service), and the other systems (\textit{e.g.} database, stream processing and machine learning) to be \textit{back-end}.

%For front-end systems, as shown in Figure~\ref{fig:socket-kernel-time}, 70\% -- 90\% CPU time is used in socket system calls. They maintain a large number of socket connections to Internet users. Establishing a socket connection takes $\approx$40$\mu$s CPU time, so a CPU core can only serve 25K new connections per second~\cite{lin2016scalable}.

%For back-end systems, the performance using socket is significantly worse than using RDMA or shared memory. This is because back-end systems interact frequently with other nodes and services, consequently the latency is vital for their performance. As shown in Figure~\ref{fig:socket-comparison}, socket latency between different processes in a same server is $\approx$10~$\mu$s, even much higher than inter-server RDMA latency ($\approx$2~$\mu$s).

%\textbf{Event-driven programming.}

%\textbf{Socket process: connection setup and data transmission.}

\subsection{High Performance Socket Systems}
\label{subsec:related-work}


There are many high performance socket systems in both academia and industry.
Table~\ref{tab:related-work} compares several of them in three dimensions: compatibility, security and performance.
The first line of research optimizes the kernel networking stack, including FastSocket~\cite{lin2016scalable}, Affinity-Accept~\cite{pesterev2012improving}, FlexSC~\cite{soares2010flexsc} and Zero-copy socket~\cite{thadani1995efficient,chu1996zero}.
Kernel optimizations achieve good compatibility and security, but capable of less performance optimizations.
MegaPipe~\cite{han2012megapipe} and Stackmap~\cite{yasukata2016stackmap} proposes new abstractions to achieve zero copy and improve I/O multiplexing, but at the cost of application modifications.

The second line of research completely bypasses or redesigns the kernel networking stack and implements TCP/IP in user space.
The user-space TCP/IP stacks use high performance packet I/O frameworks (e.g., PacketShader~\cite{han2010packetshader}, netmap~\cite{rizzo2012netmap},  DPDK~\cite{dpdk} and PF\_RING~\cite{pf-ring}) to access NIC packet queues in user space.
In this category, IX~\cite{belay2017ix} and Arrakis~\cite{peter2016arrakis} are new OS architectures that ensure security and isolation.
IX forwards every packet via the kernel to ensure performance isolation among applications, while Arrakis offloads QoS to NIC, therefore bypasses kernel for data plane.
SandStorm~\cite{marinos2014network}, mTCP~\cite{jeong2014mtcp}, Seastar~\cite{seastar}, F-Stack~\cite{fstack} proposes new abstractions to specialize network stack to applications.
LibVMA~\cite{libvma}, OpenOnload~\cite{openonload}, DBL~\cite{dbl} and LOS~\cite{huang2017high} are designed to be compatible with existing applications.
However, multiple applications of a user-space stack may share a NIC.
In order to direct received packets of listening ports and established connections to the application, each application inserts flow steering rules to the NIC independently, which is insecure and leads to conflict when multiple applications listen on a same port.
An application may also send arbitrary packets that violate access control policy.
Moreover, user-space stacks are not designed to accelerate intra-host connections and have compatibility limitations.

The third line of research utilizes RDMA NICs available in modern data centers~\cite{guo2016rdma} and translates socket operations to RDMA verbs in user space.
RDMA uses hardware offloading to provide lower latency and CPU utilization than software-based TCP/IP transport.
However, RDMA or RoCEv2 are incompatible with TCP/IP, so existing socket to RDMA systems cannot seamlessly communicate with regular TCP/IP peers.
RSocket~\cite{rsockets} and SDP~\cite{socketsdirect} converts each socket connection to an RDMA connection.
However, the RDMA NIC needs to keep track of transport states per connection with a small on-chip memory as a cache.
With more than several hundred connections, the performance degrades significantly due to cache miss~\cite{mprdma,kaminsky2016design}.
FreeFlow~\cite{freeflow} designs a software virtual switch to virtualize an RDMA NIC for container overlay network and enable intra-host RDMA communication.
It uses RSocket to translate socket to RDMA, thus limits scalability with multiple connections.
In addition, the virtual switch limits multi-core scalability.

\subsection{Performance Challenges}

We aim to deliver consistent high throughput and low latency regardless of the number of CPU cores, concurrent connections and threads.

\begin{table}[t]
	\centering
	\scalebox{0.8}{
		\begin{tabular}{l|l|l|}
			\hline
			Operation	& Latency  & Throughput  \\
			& ($\mu$s) & (M~op/s) \\
			\hline
			\hline
			Inter-core cache migration	& 0.03 & 50 \\
			\hline
			System call (before KPTI) & 0.05 & 21 \\
			\hline
			CPU L3 cache miss & 0.07 & 14 \\
			\hline
			Atomic operation & 0.10 & 5$\sim$10 \\
			\hline
			Shared memory queue & 0.25 & 27 \\
			\hline
			\textbf{Intra-host \sys} & 0.30 & 22 \\
			\hline
			System call (after KPTI) & 0.20 & 5 \\
			\hline
			Copy one page (4~KiB) & 0.40 & 5 \\
			\hline
			NIC cache miss & 0.45 & 2.2 \\
			\hline
			Cooperative context switch & 0.52 & 2.0 \\
			\hline
			CPU to NIC hairpin RDMA & 0.75 & 14 \\
			\hline
			Map 1 page (4~KiB) & 0.78 & 1.3 \\
			\hline
			Map 32 pages (128~KiB) & 1.2 & 0.8 \\
			\hline
			Two-sided inter-host RDMA & 1.6 & 8 \\
			\hline
			One-sided inter-host RDMA & 1.6 & 13 \\
			\hline
			\textbf{\sys via RDMA} & 1.6 & 8 \\
			\hline
			Semaphore, mutex, futex & 2.8$\sim$5.5 & 0.2$\sim$0.4 \\
			\hline
			Intra-host TCP & 11 & 0.9 \\
			\hline
			Copy 32 pages (128~KiB) & 13 & 0.08 \\
			\hline
			Create TCP socket & 14 & 0.07 \\
			\hline
			Inter-server TCP & 30 & 0.3 \\
			\hline
		\end{tabular}
	}
	\vspace{-10pt}
	\caption{Round-trip latency and per-core throughput of operations (testbed settings in Sec.\ref{subsec:methodology}). Message size is 8 bytes if not specified.}
	\vspace{-10pt}
	\label{tab:operation-performance}
\end{table}

\parab{Intra-host communication.}
Most existing approaches to intra-host socket are too deep into the kernel or NIC.
The kernel TCP stack has evolved to become quite complicated over the years to address performance and security concerns~\cite{yasukata2016stackmap}.
However, intra-server communication may not need many TCP features, e.g., congestion control and loss recovery.
Arrakis uses the NIC to forward packets from one application to another.
As shown in Figure~\ref{tab:operation-performance}, the hairpin latency from CPU to NIC is 25x higher than inter-core cache migration delay.
The throughput is also constrained by MMIO doorbell latency and PCIe bandwidth.

We aim to leverage shared memory queue in user space as intra-server transport.

\parab{Container overlay network.}
Many container deployments use isolated network namespaces for containers, which communicate via a virtual overlay network.
In Linux, a virtual switch daemon outside container forwards packets among host NIC and virtual NICs in containers.
This architecture incurs the overhead of multiple context switches and packet copies, and the virtual switch is a scalability bottleneck.

We aim to use the virtual switch for control plane only and implement data plane operations peer-to-peer.

\parab{Offload transport to RDMA NIC.}
For inter-host communication inside data centers, the main challenge of using RDMA is to bridge the semantics of socket and RDMA~\cite{dragojevic2014farm}.
For example, RDMA preserves messages boundaries while TCP does not.
For I/O multiplexing, RDMA provides a completion notification model while event polling in Linux socket requires a readiness model~\cite{han2012megapipe}.
Further, one-sided and two-sided RDMA verbs have different efficiency and overheads~\cite{kalia2014using,kaminsky2016design}.

We aim to use RDMA efficiently for socket applications.

\parab{Many concurrent connections.}
The vast number of connections from Internet users require cloud systems to serve millions of connections efficiently~\cite{nishtala2013scaling,lin2016scalable,belay2017ix}.
In Linux, each socket connection has dedicated send and receive buffers, each at least one page (4~KiB) in size. With millions of concurrent connections, the socket buffers can consume gigabytes of memory, most of which is empty.
Random access to a large range of buffers also leads to CPU cache and TLB miss.
This problem is amplified in RDMA NICs with limited on-chip memory.
%, which is slower than sequential access to a single buffer~\cite{li2017kv}. 
%This problem is exaggerated in RDMA. RDMA NIC caches connection states in limited on-board memory. Just over a few hundred active RDMA connections could cause cache misses and degrade performance~\cite{mprdma,kaminsky2016design}. 
%Moreover, event-driven applications use \texttt{epoll} to detect which connections are ready for send or receive. If the event queue is separated from data queues, each \texttt{recv} operation involves two cache migrations for event and data~\cite{yasukata2016stackmap}. 

We aim to multiplex socket connections and minimize memory accesses per data transmission.


\parab{Kernel bypass.}
Traditionally, socket APIs are implemented in kernel and thus require kernel crossing for each socket operation.
Recently, the Kernel Page-Table Isolation patches~\cite{kpti} to protect against Meltdown~\cite{Lipp2018meltdown} attacks make kernel crossings 4x expensive.
% (from 47~ns on Linux 4.11 to 200~ns on Linux 4.16 per system call). 
Together with additional cache invalidations, KPTI patches reduce TCP socket throughput by 40\% for 64-byte messages. %These side-channel attacks also make us rethink the Linux security model in which the complicated network stack needs to be trusted and protected. %, while sharing cores with applications.
%To ensure isolation among processes, one category of dataplane operating systems~\cite{belay2017ix,tsai2017lite} use kernel-based network stacks. However, non-batched system calls have context switch overhead, while batched system calls increase latency.

We aim to bypass kernel without compromising security.


\parab{Scale to multiple cores.}
Linux kernel acquires several global locks during connection creation~\cite{boyd2010analysis}, while MegaPipe~\cite{han2012megapipe} and FastSocket~\cite{lin2016scalable} remove these bottlenecks.
Several works~\cite{martins2014clickos,roghanchi2017ffwd,huang2017high,freeflow} delegate all operations to a virtual switch running as a daemon process, which is a scalability bottleneck.

We aim to minimize centralized coordination while preserving isolation.

\parab{Multiple threads share a connection.}
Multiple threads in a process share socket connections.
Parent and child processes after fork also share sockets.
To this end, Linux kernel acquires a per-socket lock for each socket operation~\cite{boyd2010analysis,han2012megapipe,lin2016scalable}.
Previous works~\cite{boyd2010analysis,clements2015scalable} suggest that many socket operations are not commutable and thus synchronization cannot always be avoided.

We aim to minimize synchronization overhead in two ways.
First, we optimize for the common case and remove synchronization from frequent socket operations.
Second, we leverage the fact that shared memory message passing is much cheaper than locking~\cite{roghanchi2017ffwd}, and use message passing as the exclusive synchronization mechanism.


\parab{Multiple threads share a CPU core.}
Most user-space stacks use polling to avoid the latency of interrupt and thread wakeup in the kernel.
However, polling does not work when the number of threads exceed the number of CPU cores, and multiple threads have to share a core.
To switch thread contexts, as Table~\ref{tab:operation-performance} shows, if we use semaphore, mutex or \texttt{futex} to wake up a sleeping thread, it is 5x$\sim$10x slower than cooperative context switch via \texttt{sched\_yield}.
%This extra latency is due to event notification mechanism and process scheduler in kernel.

We aim to use cooperative multitasking instead of kernel thread wakeup mechanisms.



\parab{Socket-compatible zero copy.}
The semantics of socket \texttt{send} and \texttt{recv} APIs cause memory copies between application and network stack. For \texttt{send}, the application may overwrite the memory right after \texttt{send} returns. Simply removing these copies may violate application correctness. For \texttt{recv}, application provides a buffer and read the data in buffer after \texttt{recv} returns. If we buffer data on the sender and only transmit it on \texttt{recv}, the receiver needs to wait for a round-trip time. For this reason, most socket implementations buffer data on the receiver and copy it to application on \texttt{recv}.
Many user-space sockets provide alternative APIs to achieve zero copy, but none of them can eliminate copies on both send and receive path for unmodified applications.

We aim to achieve zero copy for large transfers without changing applications. Rather than copying, we map the physical pages to new virtual addresses. However, as Table~\ref{tab:operation-performance} shows, remapping a single page is more expensive than copying it, because of kernel crossing and TLB flush. Hence, we batch page mapping operations to amortize the overhead.



%Spinlock in kernel is implemented with Compare-And-Swap (CAS) instruction, which costs $100\sim200$~ns per acquire and release. In comparison, shared memory message passing only needs one cache migration~\cite{roghanchi2017ffwd} (30~ns).
%In comparison, shared memory message passing
%Scalability analysis of Linux system calls~\cite{boyd2010analysis,clements2015scalable} suggests that many socket operations are not commutable and thus impossible to scale for all cases. 
%For example, socket provides \texttt{SO\_REUSEPORT} option. When this option is enabled, multiple processes on one host can \texttt{bind} to the same port and incoming connections are dispatched to the listeners. During connection setup, coordination is required to allocate file descriptors and port numbers, load balance connections and allocate buffers. In addition, a socket may be shared by multiple processes and threads in a same process share socket connections. When a process forks, the parent and child processes also share the sockets created before fork.


%For inter-server data transmission, Linux kernel copies data four times: on the send side, from application to kernel socket buffer, then to NIC ring buffer; the receive side is similar. For intra-server, data is copied three times: from application to kernel send buffer, then to kernel receive buffer via loopback interface, and finally to another application. Each CPU core can copy $\approx$5~GiB data per second~\cite{panda2016netbricks}, that's why a kernel TCP connection is hard to saturate an 100~Gbps NIC.
%Historically, \texttt{send} was designed as a blocking operation, and the send buffer may be overwritten by the application after \texttt{send} function call.
%In a non-blocking \texttt{send}, the process could overwrite the send buffer after \texttt{send} function call returns. To avoid race condition on the data buffer, network stack needs to copy data to an internal buffer. %before \texttt{send} returns, and then send the copied data in background.
%For the \texttt{recv} operation, application provides a buffer and read the data after \texttt{recv}.
%If we implement \texttt{send} as a lazy operation, \textit{i.e.}, the data is buffered on the sender, the receiver needs to wait for a round-trip time on \texttt{recv}.
%For this reason, most socket implementations send data to the receiver eagerly.
%Because the receiver's network stack cannot predict the user-provided \texttt{recv} address, it needs to buffer received data internally, then copy to the desired destination when \texttt{recv} is called.
%Our goal in \sys is to achieve zero copy for large data transfers without making changes to applications. We accomplish this by taking advantage of the page mapping mechanism in virtual memory. 







%\textbf{Virtual file system.}
%Each socket connection is a \textit{file descriptor} (FD) in Linux. First, each new connection involves expensive \texttt{inode} and \texttt{dentry} creation in the \texttt{proc} file system, which is not scalable and rarely used except for diagnostic and monitoring utilities. Second, socket operations need to pass through the virtual file system (VFS), which introduces CPU overhead and additional latency.





%\subsection{High Performance Socket Systems}
%\label{sec:related}


\iffalse
\begin{table*}[t]
	\centering
	\scalebox{0.88}{
		\begin{tabular}{l|c|ccc|cc|ccc|}
			\hline
			Category	& \multicolumn{1}{c|}{Linux Opt.} & \multicolumn{3}{c|}{New Kernel Stack} & \multicolumn{2}{c|}{User-space Packet} & \multicolumn{3}{c|}{User-space Socket} \\
			\hline
			System	& FastSocket & IX & MegaPipe & StackMap & Arrakis & FreeFlow & mTCP & libvma & SocksDirect \\
			\hline
			\hline
			Socket-like API & \yes & & \yes & \yes & \yes & & \yes & \yes & \yes \\
			\hline
			Linux Compatible & \yes & & & & & & & \yes & \yes \\
			\hline
			Process Isolation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes \\
			\hline
			\hline
			Kernel Bypass & & & & & \yes & \yes & \yes & \yes & \yes \\
			\hline
			Cooperative Multitasking & & & & & & & & & \yes \\
			\hline
			Zero Copy & & \yes & & \yes & \yes & \yes & & & \yes \\
			\hline
			\hline
			NIC-bypass IPC & \yes & & \yes & \yes & & \yes & \yes & & \yes \\
			\hline
			RDMA as Transport & & & & & & \yes & & & \yes \\
			\hline
			\hline
			Scalable Socket Creation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & \yes \\
			\hline
			Lock-free Multi-thread & & & & & & & & & \yes \\
			\hline
			Scale to Many Sockets & \yes & \yes & \yes & \yes & & & \yes & & \yes \\
			\hline
		\end{tabular}
	}
	\caption{Comparison of high performance socket systems.}
	\label{tab:old-related-work}
	\vspace{-10pt}
\end{table*}
\fi
