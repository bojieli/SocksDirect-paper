\section{Challenges}
\label{sec:background}

%\subsection{Socket: the Performance Bottleneck}
%\label{subsec:bottleneck}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Fraction of CPU time in the kernel (socket connection setup, socket data transmission and non-socket system calls) and user-mode applications.}
	\label{fig:socket-kernel-time}
\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Performance of back-end systems using inter-server socket, intra-server socket, inter-server RDMA and intra-server shared memory.}
%	\label{fig:backend-performance}
%\end{figure}


%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Latency comparison of mutual exclusion mechanisms (CAS, mutex, futex) and cache migration.}
%	\label{fig:mutual-exclusion}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Throughput (bar) and latency (line) for 16B messages using inter-server TCP socket, inter-server RDMA, intra-server TCP socket, UNIX socket, pipe and shared memory queue.}
%	\label{fig:socket-comparison}
%\end{figure}

\begin{table}[t]
	\centering
	\scalebox{0.9}{
		\begin{tabular}{l|l|l|}
			\hline
			Operation	& Latency  & Throughput  \\
						& ($\mu$s) & (M~op/s) \\
			\hline
			\hline
			Inter-core cache migration	& 0.03 & 50 \\
			\hline
			System call (Linux 4.11) & 0.05 & 21 \\
			\hline
			CPU L3 cache miss & 0.07 & 14 \\
			\hline
			Atomic operation & 0.10 & 5$\sim$10 \\
			\hline
			Shared memory queue & 0.12 & 27 \\
			\hline
			Intra-server SocksDirect & 0.15 & 19 \\
			\hline
			System call (Linux 4.16) & 0.20 & 5 \\
			\hline
			NIC cache miss & 0.45 & 2.2 \\
			\hline
			Cooperative context switch & 0.52 & 2.0 \\
			\hline
			Two-sided RDMA & 1.0 & 8 \\
			\hline
			One-sided RDMA & 1.0 & 23 \\
			\hline
			SocksDirect via RDMA & 1.0 & X \\
			\hline
			Inter-core hairpin via NIC & 1.8 & 5 \\
			\hline
			libvma~\cite{libvma} TCP socket & 2.7 & 5 \\
			\hline
			Remap 32 pages (128~KiB) & 2.9 & 0.34 \\
			\hline
			Semaphore, mutex, futex & 2.8$\sim$5.5 & 0.2$\sim$0.4 \\
			\hline
			Intra-server pipe (4.16) & 4.0 & 2.1 \\
			\hline
			Intra-server TCP (4.11) & 8.0 & 3.4 \\
			\hline
			Intra-server TCP (4.16) & 8.5 & 2.0 \\
			\hline
			Inter-server TCP socket & 10 & 0.3 \\
			\hline
			128~KiB memory copy & 15 & 0.07 \\
			\hline
			Create socket connection & 35 & 0.03 \\
			\hline
		\end{tabular}
	}
	\caption{Per-core throughput and one-way latency of operations on our testbed (settings in Sec.\ref{sec:evaluation}). Message size is 64 bytes if not specified.}
	\label{tab:throughput-latency}
\end{table}


Linux kernel socket is a well-known bottleneck for communication intensive applications. We stress test Nginx load balancer, memcached and Redis key-value stores with one request per TCP connection or a stream of requests on pre-established TCP connections. As shown in Figure~\ref{fig:socket-kernel-time}, 50\%$\sim$90\% CPU time is consumed in the kernel, mostly dealing with socket system calls. Connection setup is especially slow, causing the one request per connection scenario shows much lower throughput. If we were able to mitigate the overhead associated with sockets, application performance would be 2x$\sim$10x.
%As another example, we replaced socket with RDMA and enabled zero copy gRPC in distributed Tensorflow~\cite{abadi2016tensorflow}, and training a VGG network shows 5x speedup.


%Socket is a well-known bottleneck in fast data center networks \RED{(cite)}. In a data center, we categorize systems that process queries from Internet users to be \textit{front-end} (\textit{e.g.} DNS server, load balancer and Web service), and the other systems (\textit{e.g.} database, stream processing and machine learning) to be \textit{back-end}.

%For front-end systems, as shown in Figure~\ref{fig:socket-kernel-time}, 70\% -- 90\% CPU time is used in socket system calls. They maintain a large number of socket connections to Internet users. Establishing a socket connection takes $\approx$40$\mu$s CPU time, so a CPU core can only serve 25K new connections per second~\cite{lin2016scalable}.

%For back-end systems, the performance using socket is significantly worse than using RDMA or shared memory. This is because back-end systems interact frequently with other nodes and services, consequently the latency is vital for their performance. As shown in Figure~\ref{fig:socket-comparison}, socket latency between different processes in a same server is $\approx$10~$\mu$s, even much higher than inter-server RDMA latency ($\approx$2~$\mu$s).

%\textbf{Event-driven programming.}

%\textbf{Socket process: connection setup and data transmission.}


To simplify deployment, we design \sys to be fully \textit{compatible} with existing applications using Linux socket. \textit{Process isolation} needs to be preserved. In addition, \sys has four performance goals. First, \textit{low latency}. Second, \textit{high throughput} (aka low CPU overhead). For inter-process connections within a server, the latency and throughput should be comparable with shared memory communication. For inter-server connections in a data center, we aim at performance close to RDMA. Third, the performance should be \textit{scalable to multiple cores}. Finally, the performance should not degrade with \textit{many concurrent connections}.
%The source of socket overhead has been analyzed extensively~\cite{peter2016arrakis,lin2016scalable}.
In the following, we analyze challenges to meet the performance goals while preserving compatibility.

\textbf{Kernel crossing.}
Socket APIs are implemented in kernel space and thus require kernel crossing for each socket operation. Recently, the KPTI patches~\cite{kpti} to protect against Spectre~\cite{Kocher2018spectre} and Meltdown~\cite{Lipp2018meltdown} attacks make kernel crossings 4x expensive (from 47~ns on Linux 4.11 to 200~ns on Linux 4.16 per system call). Together with additional cache invalidations, KPTI patches reduce kernel TCP socket 64-byte message throughput by 40\%. These side-channel attacks also lead us to rethink Linux security model that the complicated network stack needs to be trusted and protected, yet share cores with applications.

To ensure isolation among processes, one category of dataplane operating systems~\cite{belay2017ix,tsai2017lite} use kernel-based network stacks. However, non-batched system calls have context switch overhead, while batched system calls increase latency. Another line of work~\cite{martins2014clickos,roghanchi2017ffwd,huang2017high} delegate socket processing to a virtual switch running on dedicated cores. The throughput of virtual switch becomes a bottleneck. Our goal is to achieve kernel bypass with process isolation, while not delegating every operation to a centralized coordinator.

\textbf{Context switch and process scheduling.}
Switching from one process to another cooperatively ($0.52\mu$s) is 2.5x expensive than a system call. Inter-process context switch based on semaphore, mutex or \texttt{futex} takes $2.8\sim5.5\mu$s, even 5x$\sim$10x slower than cooperative multitasking. This additional latency attributes to event notification mechanism and scheduler queue manipulation in kernel. For two processes that pin on different cores, one-way TCP socket takes $\approx8\mu$s, half of which attributes to the same reason of semaphore delay. In this regard, low latency networking and IPC should use cooperative multitasking rather than kernel wakeup.

\textbf{Synchronization.}
Scalability analysis of Linux system calls~\cite{boyd2010analysis} suggests that many socket operations are not commutable. During connection setup, we need to create file descriptors, load balance \texttt{connect} requests and allocate buffers. In addition a socket connection may be shared by multiple processes and threads. Threads in a same process share all socket connections. When a process forks, the parent and child processes share the sockets created before fork. The processes and threads sharing a socket forms a multi-producer and multi-consumer model. Logically, senders enqueue data into a shared FIFO, and receivers dequeue an application-specified number of bytes from the FIFO.

Linux kernel acquires global locks during connection setup and a per-socket lock for each socket operation, limiting socket scalability~\cite{boyd2010analysis,han2012megapipe,lin2016scalable}. In fact, even in non-contending scenarios, synchronization still introduces significant overhead. Mutual exclusion is typically implemented with Compare-And-Swap (CAS) instruction, which costs $100\sim200$~ns per lock and unlock. In comparison, inter-core message passing can be implemented with only one cache migration~\cite{roghanchi2017ffwd} (30~ns).

\textbf{Memory copy.}
For inter-server data transmission, Linux kernel copies data four times: on the send side, from application to kernel socket buffer, then to NIC ring buffer; the receive side is similar. For intra-server, data is copied three times: from application to kernel send buffer, then to kernel receive buffer via loopback interface, and finally to another application. Each CPU core can copy $\approx$5~GiB data per second~\cite{panda2016netbricks}, that's why a kernel TCP connection is hard to saturate an 100~Gbps NIC.

However, avoiding data copy is not simple for compatibility with Linux socket. Historically, \texttt{send} was designed as a blocking operation, so the send buffer may be overwritten by the application after \texttt{send} operation.
Non-blocking \texttt{send} preserves this semantics, so the network stack needs to copy data to an internal buffer, then send the internal buffer in background after \texttt{send} returns.
For the \texttt{recv} operation, application provides a buffer and read the data after \texttt{recv} returns.
If we implement \texttt{send} as a lazy operation, \textit{i.e.}, the data is buffered on the sender, the receiver needs to wait for a round-trip time on \texttt{recv}.
For this reason, most socket implementations send data to the receiver eagerly. Because the receiver's network stack cannot predict the user-provided \texttt{recv} address, it needs to buffer received data internally, then copy the data when \texttt{recv} is called.
Our goal is to achieve zero copy in most large data transfers without change to applications.

\textbf{Cache miss.}
The vast number of WAN connections from Internet users require cloud systems to serve millions of connections efficiently~\cite{nishtala2013scaling,lin2016scalable,belay2017ix}, called the C10M problem~\cite{graham2013c10m}. In Linux, each socket connection has dedicated send and receive buffers, and each buffer is at least one page (4~KiB). With one million concurrent connections, the socket buffers consumes tens of gigabytes of memory, most of which is empty. Accessing many buffers randomly also leads to non-cached memory accesses, which is slower than sequential access to a single buffer~\cite{li2017kv}. In addition, event-driven applications use \texttt{epoll} to know which connections are ready for send or receive. If the event queue is separated from data queues, each \texttt{recv} operation involves two cache migrations for event and data~\cite{yasukata2016stackmap}. In light of this, we need to reduce memory accesses per data transmission.

%\textbf{Virtual file system.}
%Each socket connection is a \textit{file descriptor} (FD) in Linux. First, each new connection involves expensive \texttt{inode} and \texttt{dentry} creation in the \texttt{proc} file system, which is not scalable and rarely used except for diagnostic and monitoring utilities. Second, socket operations need to pass through the virtual file system (VFS), which introduces CPU overhead and additional latency.

\textbf{TCP Transport, QoS and ACL.}
Linux TCP implementation has evolved to be complicated over the years to address performance and security concerns, as well as incorporate extensions~\cite{yasukata2016stackmap}. However, intra-server sockets do not need the ordering guarantee, loss recovery and congestion control offered by TCP. QoS and performance isolation are also not required because inter-core communication in a CPU can scale with number of cores without a bottleneck~\cite{intel-manual}. For inter-server communication, RDMA offers reliable transmission in NIC hardware and firmware. An emerging line of work~\cite{zhu2015congestion,guo2016rdma,lu2017memory,mprdma} improve RDMA performance and QoS in large scale data center networks. In this work, we consider RDMA a mature technology in data centers. We take the idea of Arrakis~\cite{peter2016arrakis} and Azure SmartNICs~\cite{smartnic} to enforce QoS and ACL policies on NICs.

The main challenge is bridging the semantics of socket and RDMA. The event polling API in Linux is \textit{reactive}, in the sense that the applications knows a socket connection is ready for \texttt{send} or \texttt{recv}, then issues the operation.
On the contrary, RDMA API is \textit{proactive}. The application delegates RDMA operations to the NIC hardware, and receives notification when they are completed.
First, two-sided RDMA applications needs to predict the size and number of messages it expects to receive, and delegate enough \texttt{recv} operations to the NIC~\cite{huang2017high}.
Second, because RDMA NIC caches connection states in limited memory, a large number of concurrent RDMA connections would cause frequent cache misses and degrade performance~\cite{mprdma,kaminsky2016design}.
Our goal is to use RDMA efficiently for existing socket applications.
