\section{Inter-Process Socket in a Server}
\label{sec:intra-server}

\subsection{Processes as Distributed System}
\label{subsec:socket-api}

How the socket works. A table show the process of using socket:

\begin{itemize}
	\item Process and thread creation. (fork, pthread\_create)
	\item Socket initialization.
	\item Connection setup.
	\item Data transmission.
\end{itemize}


Data structure in each process. Data structure in the monitor process.

Need a figure to show the lifecycle (message passing flow) of a socket connection. Three sub-figures:


	 \textbf{Socket initialization.}
	  
	 \textbf{Connection setup. (accept scheduling)}
	 
	 \textbf{Data transmission.}
	 
	 Discuss epoll.
\begin{itemize}
	\item Pick from the middle of the Q
	\item Starving (per fd slot)
	\item emergency Q
\end{itemize}

    \textbf{Close \& ACK}


\subsection{Scaling socket API}
\label{subsec:fork}

Process fork and thread creation are essential methods to enable parallelism in modern applications. 
However, as stated in \ref{subsec:challenges}, fork and thread creation pose the contention of logically shared data since it is a producer-consumer model and multiple consumer compete one shared data. With traditional mutual exclusion solution, the scalability of Socket is limited.

To tackle this issue, we make the following observation.
\begin{itemize}
	\item Fork system call and thread creation is low-frequency used in high performance applications.
	\item It is uncommon that several threads read data from one connection concurrently since Linux cannot guarantee atomicity of the bytes stream.
\end{itemize} 

Our design of multiple threading can be divided to three parts: a) read and write operation b) fork and pthread\_create function call (i.e. duplicate a fd to two processes). c) close function call (i.e. destroy a fd). Based on the observation above, our goal is to maximize the performance of the usual case while keep the maximum compatibility of the Linux socket semantics.

For the performance goal, base on the second observation. We propose the following requirements to maximize the performance of the usual case:
\begin{itemize}
 \item In multiple threads scenario, the performance of one-one connection ought to have the similar throughput and latency compared with that in single thread scenario.
 \item For multiple sender and one receiver, the performance is supposed to scale, rather than dropped heavily compared with mutual exclusion solution.
 \item  
\end{itemize}


One instinctive design to archive the goal is to create separate queues for each sender and receiver and the queues build a bipartite graph between the sender and the receiver. Senders send the message to different receivers in turn. We could prove that from the receiver side, the order of one designated sender is ensured.

By adopting the methods above, we illustrate the process of fork function call as figure \ref{fig:fork-process}. When fork is called, the process notify monitor and monitor create new shared buffer for each existing shared buffer between the parent process and the existing peers of parent process. The peer of the parent process is notified of the fork and connect to the newly created child process.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{This figure should explain the workflow when fork happens.}
	\label{fig:fork-process}
\end{figure}

While the previous design achieves high scalability compared with traditional Linux socket design, but there are several corner cases that our design is not compatible with the Linux.

1. Before the new thread is created, the queue of a connection may already accumulate some bytes. After the fork, how could we deal with the existing data in the old queue? One simple solution is keep the data in the old queue, but by doing so, newly created process cannot access the data of the old queue, which violate the Linux semantics: all the data is shared by all the receivers. 

2. The old process or the new process may not consume the data of the connection (i.e. dead receiver). For instance, Nginx creates a socket for log propose and only one process read from the socket while all the threads have the access to that file descriptor. However, without this knowledge, sender side consistently push data into all the receivers, which lead to starving and deadlock (i.e. the queue of the dead receiver is full).

To settle the first problem, we take advantage of a tree data structure. To settle the second problem, we take idea of a pointer in shared memory and the takeover concept.

\subsubsection{title}  



Key point: Coordination in shared socket.

Clock pointer (simple)

Takeover (compare with \ref{subsec:socket-api})

Read Lock

Tree

Fork \& Close interleaving

\subsection{Shared-Memory Queue}

x86-TSO~\cite{sewell2010x86}, intel manual~\cite{intel-manual}

\subsection{Process Multiplexing}
\label{subsec:epoll}


Message passing may wait too long.
Signal mechanism. Signal process after waiting until timeout.

Monitor detect process death (via control socket SIGHUP) and notify endpoints.

Blocking operation: sched\_yield.


\subsection{Zero Copy}
\label{subsec:zerocopy}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Zero-copy theory of operation.}
	\label{fig:zerocopy}
\end{figure}

The main challenge for zero copy is the buffer reuse after send and the non-deterministic buffer for receive.
Fortunately, the virtual memory provides a layer of indirection, so we can remap virtual address of a buffer to another physical page, if the data occupies entire 4~KiB pages.
To this end, we wrap around \texttt{malloc} function and allocate 4~KiB aligned addresses for large allocations.
If the size of send message is not a multiple of 4~KiB, the last chunk of data is copied on \texttt{send} and \texttt{recv}.

As shown in Figure~\ref{fig:zerocopy}, for \texttt{send} operation, \libipc{} invokes the kernel to get an encrypted physical address of send buffer and send the address to receiver via user-mode shared-memory queue.
The address is encrypted to prevent unsolicited mapping of arbitrary pages.
Because the sender may read the buffer after \texttt{send} or send the buffer to multiple receivers, the physical page cannot be remapped.
Additionally, \texttt{send} needs to write-protect the buffer because the receiver needs to read it.
On receiving side, \libipc{} invokes the kernel to remap the encrypted physical address to the application-provided receive buffer.
\texttt{recv} also needs to write-protect the buffer because it is shared by the sender and potentially multiple receivers.

A challenge arises when sender writes the buffer after \texttt{send}.
Existing zero-copy socket designs~\cite{thadani1995efficient,chu1996zero} use copy-on-write. Copy is required because the sender may read the non-written part of the page.
Because most applications reuse the buffer for subsequent send operations, copy-on-write is invoked in most cases, making zero-copy essentially useless on sender.
Our observation is that most applications overwrite entire pages of the send buffer via \texttt{recv} or \texttt{memcpy}, so it is unnecessary to copy old data when the first byte of the page is written.
Zero-copy \texttt{recv} remaps the buffer without triggering copy-on-write, so proxy applications can achieve zero copy.
For \texttt{memcpy}, we add preamble code to the function in both \libipc{} runtime and compiler inline library. If both source, destination addresses and the copy size are aligned to 4~KiB, and the destination address is a send buffer, the preamble code invokes the kernel to remap new pages to the destination address and disable write protection.
For compatibility, copy-on-write is still used for other write operations to protected buffers.

A second challenge is that page remapping requires the kernel to allocate and free pages for each \texttt{send} and \texttt{recv}. Page allocation in kernel acquires a global lock, therefore it is inefficient. Instead, \libipc{} manages a pool of free pages in each user-mode process.
We add to the kernel a system call to convert virtual addresses in the pool and encrypted physical addresses.
\libipc{} also tracks the origin of received zero-copy buffers.
After page remapping on sender \texttt{memcpy} and receiver \texttt{recv}, if the old physical page is from another process, \libipc{} sends a message to the origin process to return the buffer.


\section{Utilizing RDMA for Inter-server Socket}
\label{sec:rdma}

Remember: if the other endpoint is not RDMA capable, use libvma.
