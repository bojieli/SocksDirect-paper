\section{Inter-Process Socket in a Server}
\label{sec:intra-server}

\subsection{Processes as Distributed System}
\label{subsec:socket-api}

How the socket works. A table show the process of using socket:

\begin{itemize}
	\item Process and thread creation. (fork, pthread\_create)
	\item Socket initialization.
	\item Connection setup.
	\item Data transmission.
\end{itemize}


Data structure in each process. Data structure in the monitor process.

Need a figure to show the lifecycle (message passing flow) of a socket connection. Three sub-figures:


	 \textbf{Socket initialization.}
	  
	 \textbf{Connection setup. (accept scheduling)}
	 
	 \textbf{Data transmission.}
	 
	 Discuss epoll.
\begin{itemize}
	\item Pick from the middle of the Q
	\item Starving (per fd slot)
	\item emergency Q
\end{itemize}

    \textbf{Close \& ACK}


\subsection{Scaling Socket API}
\label{subsec:fork}

Process fork and thread creation are essential mechanisms to enable parallelism in modern applications. 
However, as stated in \ref{subsec:challenges}, fork and thread creation makes socket a producer-consumer model, where senders and receivers contend on the shared queue. With a traditional mutual exclusion solution, the multi-process scalability of socket is limited.

To tackle this issue, we make the following observations:
\begin{enumerate}
	\item Fork and thread creation are not frequent in high performance applications, compared to connection setup and data transmission.
	\item It is uncommon that several processes receive concurrently from one shared socket.
\end{enumerate} 

Based on the observations above, our goal is to maximize the common-case performance while keep the maximum compatibility of the Linux socket semantics.
Based on the second observation. We propose the following requirements to maximize the common-case performance:
\begin{enumerate}
 \item \textbf{Synchronization-free.} With multiple potential senders and receivers, if only one pair of sender and receiver is active, the throughput and latency should be comparable with that of a sender-receiver pair.
 \item \textbf{Scalability.} For multiple active senders and one receiver, if the receiver is not the bottleneck, the throughput is supposed to scale, rather than dropping heavily due to mutual exclusion.
 \item \textbf{Self-stabilization.} Previous access patterns shall not have follow-up effect on current performance. Corner-case operations may slow down the system temporarily, but after that the throughput is supposed to converge to common-case.
\end{enumerate}

For compatibility of Linux semantics, the fundamental requirements is the order of messages. To be specific:
\begin{enumerate}
\item \textbf{Single receiver ordering.} For a specific consumer, the order of messages it from the same producer is supposed to be guaranteed.
\item \textbf{Multiple receiver ordering.} The order of messages sent by one sender and received by multiple receiver should be guaranteed. e.g. For any messages D1, D2 sent by a sender S,  we guarantee that receiver R1 gets D1 and receiver R2 gets D2 if read called by R1 is prior to that by R2. 
\item \textbf{Deadlock-free.} Our design shall not run into deadlock. i.e. The existence of data of a certain connection and the existence of pending read function call cannot occur simultaneity.
\item \textbf{Starvation-free.} Starvation is not supposed to happen in our system. i.e. One sender transmit data to two consumer, but only one receiver can get the message while the other one keeps waiting.
\end{enumerate}

Briefly, our design of multiple processing can be divided to three parts: a) read and write operation b) fork and pthread\_create function call (i.e. new processes add to the existing connection). c) close function call (i.e. existing connection is closed, all the processes leave the connection).


\subsubsection{Read/Write Operation}
\label{subsubsec:fork_rdwr}

In this paragraph, we assume that the connection is already setup and the number of processes involved in this connection is stable. In order to achieve high throughput, we leverage lock-free queue to enable one-one connection. For multiple process on one connection, in order to achieve high scalability, we create separate queues for each sender and receiver and the queues build a bipartite graph between the sender and the receiver.To solve the multi-receiver ordering requirement and the deadlock-free requirement, each sender of a socket designate one receiver with exclusive access to the buffer. The receiver round robin poll the messages from all the queues connected to senders. Figure \ref{fig:fork-bipartitegraph} shows a sample of the shared-memory buffers between senders and receivers for one connection. 

How to choose the specific receiver of the sender is a key challenge since the receiver chose by the sender may not handle the data in that connection while other receivers may under starvation. Based on our observation that usually only one receiver retrieves data from the sender side, we could simply let the sender transmit the data to the receiver who previously requested for it i.e. the receiver which calls read first send a ``takeover message'' to sender and sender begins to transmit byte stream to that receiver.  The main reason for takeover message is the multi-receiver ordering requirement and the deadlock-free requirement. In common cases, the performance could be achieved as high as one-one connection since the sender only send the data to one receiver and no contention happens in usual cases.

However, in this design, in order to satisfy the order requirements, how to deal with the data already in the queue while a different receiver sends ``takeover message'' is especially challenging.  We need to ensure that all the old data is read by the newly receiver first, and then the new data of sender can be sent to the that receiver. Because multiple socket connections share a queue from sender to receiver, the new receiver cannot take over the queue from the old receiver directly. Instead, the sender first send a takeover request to the old receiver. Upon receiving of the takeover request, the old receiver returns all unconsumed messages in queue to sender, then sender forwards the messages to the new receiver. To tackle this problem, sender side first send a request to the old receiver to request it return all the unconsumed messages in queue to sender and sender retransmit them to new receiver. During migration of unconsumed messages, the sender blocks send operations to ensure message ordering.

Another challenge is that when a receiver should send a takeover message. In order to let the receiver know whether itself is designated by the sender, the sender keeps a bit flag in the shared memory between it and each receiver.  Only one of these flags, which is in where sender gets the ``takeover message'' lastly is set to one while other flags are set to zero. When sender notices a ``takeover message'', it changes the flag linked with the new receiver to one and set the original flag to zero.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{This figure shows a stable connection handled by multiple senders and receivers.}
	\label{fig:fork-bipartitegraph}
\end{figure}

\subsubsection{Fork and Thread creation}
Fork is a procedure that new processes attach to existing connections. Logically, there are four different cases when fork happens for one process, noted as 1, 2, 3, 4 in the following of the paragraph:
\begin{enumerate}
	\item The process itself is the sender side of the connection and it forks.
	\item The process itself is the sender of the connection while one of the receiver folks.
	\item The process itself is the receiver side of the connection and it forks.
	\item The process itself is the receiver of the connection while one of the sender folks.
\end{enumerate}

The general process of fork is that after monitor is notified of the fork, it creates shared memory between the newly created process and all the processes which previously have connections with the parent process. The key challenge lies in the fork is that how to deal with the existing data in the connection to guarantee the order requirements.

For the cases 2 and 3, we could adopt the idea of ``takeover message'' as stated in \ref{subsubsec:fork_rdwr}. Each of the forked new process creates a new queue for its old connection. When the receiver side call read for the connection, a ``takeover message'' is emit and the sender designate the new process as the exclusive receiver and poll all the data in the old queue to the receiver which sends ``takeover message''.

However, It is more complicated for case 1 and case 4. We cannot leverage the ``takeover message'' trick since the two senders may emit data in parallel and we cannot guarantee all the data sent prior to fork is consumed before to that following the fork by new processes. It is not efficient to block all the senders before old data is consumed either.

Our solution is to when the sender side forks, we preserve the old shared memory and create two new shared memory for each receiver and connection them to the new sender processes separately. All the receiver side is notified of the folk and keep a tree data structure of all the shared memories.The two new sender transmit all the new messages to the newly created shared memory separately. When the receiver tries to retrieve data, it first try the old shared memory, if it is empty, it will remove the old shared memory and promote the child of this memory for the read request. 

Things become much more complicated when cases 1,4 happens after cases 2,3 happening. After receiver forks, the unique sender is responsible for receiving ``takeover message'' and resend the data to new receivers. However, if sender forks following the receiver forks, according to the methods we mentioned above, there is no sender responsible for processing ``takeover message''. Our solution is that we require the receivers to poll the data from the old shared memory queue and compete for data. Since this case rarely happens, the performance of the overall design is not affected.

\subsubsection{Connection Close}

Close is the operation that all of the processes leave the connection. The synchronization is  especially challenging since all the processes are run in parallel. e.g. One process close a connection while the others are doing compute intensive tasks. It is possible that the file descriptor of the old process is reused and a new connection is setup with the same file descriptor. The other process may notice the close of the old connection and also call close on its own side, which lead to the new connection setup by the previous process closed due to the match of same file descriptor. 

To satisfy the synchronization requirements, the close function call is all completed by message passing. The caller of close need to wait for ACK from all the other peers before release resources. i.e. the status of the connection.

Another challenge is that different kinds of messages may interleaved. One case is that is a process waiting for ACK of close request may get a fork notification by the peer.
  
Key point: Coordination in shared socket.

Clock pointer (simple)

Takeover (compare with \ref{subsec:socket-api})

Read Lock

Tree

Fork \& Close interleaving

\subsection{Shared-Memory Queue}

x86-TSO~\cite{sewell2010x86}, intel manual~\cite{intel-manual}

\subsection{Process Multiplexing}
\label{subsec:epoll}


Message passing may wait too long.
Signal mechanism. Signal process after waiting until timeout.

Monitor detect process death (via control socket SIGHUP) and notify endpoints.

Blocking operation: sched\_yield.


\subsection{Zero Copy}
\label{subsec:zerocopy}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{images/fixme}
	\caption{Zero-copy theory of operation.}
	\label{fig:zerocopy}
\end{figure}

The main challenge for zero copy is to maintain the semantics of socket API. The sender may write the send buffer after non-blocking \texttt{send}, and the receiver does not know the receive buffer before \texttt{recv}.
Fortunately, the virtual memory provides a layer of indirection, so we can remap virtual address of a buffer to another physical page, if the data occupies entire 4~KiB pages.
To this end, we wrap around \texttt{malloc} function and allocate 4~KiB aligned addresses for large allocations, so most buffers will align to page boundary.
If the size of send message is not a multiple of 4~KiB, the last chunk of data is copied on \texttt{send} and \texttt{recv}.

As shown in Figure~\ref{fig:zerocopy}, for \texttt{send} operation, \libipc{} invokes the kernel to get an encrypted physical address of send buffer and send the address to receiver via user-mode shared-memory queue.
The address is encrypted to prevent unsolicited mapping of arbitrary pages.
Because the sender may read the buffer after \texttt{send} or send the buffer to multiple receivers, the physical page cannot be remapped.
Additionally, \texttt{send} needs to write-protect the buffer because the receiver needs to read it.
On receiving side, \libipc{} invokes the kernel to remap the encrypted physical address to the application-provided receive buffer.
\texttt{recv} also needs to write-protect the buffer because it is shared by the sender and potentially multiple receivers.

A challenge arises when sender writes the buffer after \texttt{send}.
Existing zero-copy socket designs~\cite{thadani1995efficient,chu1996zero} use copy-on-write. Copy is required because the sender may read the non-written part of the page.
Because most applications reuse the buffer for subsequent send operations, copy-on-write is invoked in most cases, making zero-copy essentially useless on sender.
Our observation is that most applications overwrite entire pages of the send buffer via \texttt{recv} or \texttt{memcpy}, so it is unnecessary to copy old data when the first byte of the page is written.
Zero-copy \texttt{recv} remaps the buffer without triggering copy-on-write, so proxy applications can achieve zero copy.
For \texttt{memcpy}, we add preamble code to the function in both \libipc{} runtime and compiler inline library. If both source, destination addresses and the copy size are aligned to 4~KiB, and the destination address is write-protected by \libipc{}, the preamble code invokes the kernel to remap new pages to the destination address and disable write protection.
For compatibility, copy-on-write is still used for other write operations to protected buffers.

A second challenge is that page remapping requires the kernel to allocate and free pages for each \texttt{send} and \texttt{recv}. Page allocation in kernel acquires a global lock, therefore it is inefficient. Instead, \libipc{} manages a pool of free pages in each user-mode process.
We add to the kernel a system call to convert virtual addresses in the pool and encrypted physical addresses.
\libipc{} also tracks the origin of received zero-copy buffers.
After page remapping on sender \texttt{memcpy} and receiver \texttt{recv}, if the old physical page is from another process, \libipc{} sends a message to the origin process to return the buffer.


\section{Inter-Server Socket}
\label{sec:rdma}

RDMA if the remote side supports socket.

 libvma (lightweight user-mode stack).

Remember: if the other endpoint is not RDMA capable, use libvma.

 It reserves the send buffer after \texttt{send} and releases it after send completion notification.
The application also delegates non-blocking receive operations along with receive buffers, so the NIC can DMA received data directly to the receive buffers, then notify the application that data is ready.