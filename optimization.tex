\section{Optimization}
\label{sec:optimization}

This section addresses two performance challenges: How to achieve socket-compatible zero copy? How to reduce process wakeup cost in kernel?



\subsection{Zero Copy}
\label{subsec:zerocopy}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/zerocopy}
%	\caption{Zero copy via page remapping. Step 1: \texttt{send}, sender get physical address and send via shared memory. Step 2: \texttt{recv}, receiver maps the page to buffer. Step 3: sender remaps buffer on memory write.}
%	\vspace{-15pt}
%	\label{fig:zerocopy}
%\end{figure}

\RED{Need a performance comparison figure for copy, batch page remapping, single page remapping, etc.}

The main challenge for zero copy is to maintain the semantics of socket API.
%A sender may write the send buffer after non-blocking \texttt{send}, and the receiver does not know the receive buffer before \texttt{recv}.
Fortunately, virtual memory provides a layer of indirection. % so we can remap virtual address of a buffer to another physical page if the data occupies entire 4~KiB pages.
Rather than copying, we remap the physical pages from sender's virtual addresses to receiver's.
To enable zero-copy, we need to modify the NIC driver to expose several kernel functions related to page remapping. 
%Due to the remapping overhead (Table~\ref{tab:operation-performance}), we only use zero-copy for \texttt{send} or \texttt{recv} with at least 8~KiB payload size. Smaller messages are copied instead.
As Table~\ref{tab:operation-performance} shows, remapping a single page is more expensive than copying it because of kernel crossing and TLB flush costs. Hence, we only use zero copy for \texttt{send} or \texttt{recv} with at least 8~KiB payload size.
Smaller messages are copied instead.
Page remapping has been used by zero-copy sockets~\cite{thadani1995efficient,chu1996zero,linux-zero-copy} and we make following improvements.

\parab{Page alignment.}
Page remapping only works when the send and receive addresses are page aligned and the transfer contains entire pages.
We intercept \texttt{malloc} and \texttt{realloc} functions and allocate 4~KiB aligned addresses for allocations with multiple-of-4K sizes, so most buffers will align to page boundary, while not wasting memory for small allocations.
If the size of sent message is not a multiple of 4~KiB, the last chunk of data is copied on \texttt{send} and \texttt{recv}.

%\parab{Amortize page remapping cost.}


\parab{Minimize copy-on-write.}
%After \texttt{send}, because the application may read the buffer or send the buffer to other receivers, it needs to write-protect the buffer.
%\texttt{recv} also needs to write-protect received buffers.
When sender writes the buffer after \texttt{send}, existing designs use copy-on-write. %Copy is required because the sender may read the non-written part of the page.
Because applications almost always reuse the buffer for subsequent send operations, copy-on-write is invoked in most cases, making zero-copy essentially useless on sender.
Our observation is that most applications do not modify the buffer. Instead, they overwrite entire pages of the send buffer via \texttt{recv} or \texttt{memcpy}. %so it is unnecessary to copy old data when the first byte of the page is written.
%For \texttt{recv}, the old read-only mapping can be safely discarded.
For \texttt{recv}, the zero-copy page remapping mechanism already handles the case and the received buffer can be safely used for send. 
For \texttt{memcpy}, we add preamble code to \texttt{memcpy}, so that
for page-aligned copy to \libipc{} buffers, the preamble code invokes the kernel to remap new pages and disables write protection.

\parab{Page allocation overhead.}
Page remapping requires the kernel to allocate and free pages for each zero copy \texttt{send} and \texttt{recv}. 
Page allocation in kernel uses a global lock, which is inefficient. \libipc{} manages a pool of free pages in each process locally.
\libipc{} also tracks the origin of received zero-copy pages.
Whenever a page is remapped, if it is from another process, \libipc{} return the pages to the owner through a message.

\parab{Send page addresses securely in user space.}
For intra-host socket, we send the physical page addresses in a message in user-space queues.
We must prevent unsolicited remapping of arbitrary pages.
To this end, \libipc{} invokes a modified NIC driver to 
get obfuscated physical page addresses of the send buffer and send the address to receiver via shared memory queue.
On the receiving side, \libipc{} invokes the kernel to remap the obfuscated physical pages to the application-provided receive buffer virtual address.

\parab{Zero Copy under RDMA.}
\libipc{} initializes a pinned page pool on receiver and send the physical addresses of the pages to the sender to manage and use.
On sender, \libipc{} %pins the send buffer if it has not been pinned, then 
allocates pages from the remote receiver page pool to determine the remote address of RDMA write.
On receiver, when \texttt{recv} is called, \libipc invokes the NIC driver to map pages in the pool to application buffer virtual address.
After the remapped pages are freed (e.g. overwritten by another \texttt{recv}), \libipc{} returns them to the pool manager in sender through message.
%If the OS runs out of memory, \libipc{} unpins pages to reclaim memory.
%For security, kernel validates that page numbers are in the huge-page receive buffer.

%\subsubsection{Zero Copy TCP}
%\label{subsec:zero-copy-tcp}
%
%For TCP connections, we optimize the user-space TCP/IP stack to remove memory copy between \libipc{} and NIC.
%Because the payloads of sent and received packets need to align at 4~KiB page boundary, we leverage scatter-gather support in modern NICs~\cite{mellanox} to separate packet header from application payload.
%%During initialization, \libipc{} queries IP and Ethernet MAC address from the kernel and constructs a packet header template.
%For \texttt{send}, \libipc{} constructs a packet header to a NIC send work request, then fills in the payload buffer address from application. 
%For receiving data, in background, \libipc{} issues NIC receive work requests with a 54-byte buffer to store Ethernet, IPv4 and TCP headers, followed by a page-aligned buffer to store payload.
%%In corner cases where the received header length is not 54 bytes, \libipc{} reassembles the packet.
%Upon \texttt{recv}, the payload buffer is remapped to application.

\subsection{Cooperative Multitasking}
\label{subsec:process-mux}

To deal with the scenario where multiple threads share a CPU core, rather than using OS thread wakeup, we use cooperative multitasking to switch thread contexts efficiently.


\parab{Event notification.}
%To minimize context switch, \sys{} runs in user mode and uses cooperative multitasking to multiplex processes on CPU cores. 
Coordination and delegation based on message passing requires processes to respond to messages promptly. However, processes may execute application code without calling \libipc{} for a long time. To address this issue, we design a \textit{signal} mechanism analogous to interrupts in operating systems. Event initiators  first poll the receive queue for a period of time for ACK. If no reply, it sends a Linux \texttt{signal} to the receptor and wake up the process.

The signal handler, registered by \libipc{}, first determines whether the process is executing application or \libipc{} code. \libipc{} sets and clears a flag at entry and exit of the library. If signal handler finds that the process is in \libipc, it does nothing and \libipc{} will process the event before returning control to the application. Otherwise, the signal handler immediately processes messages from the emergency queue to the monitor, before returning control to the application. 
%Because \libipc{} is designed to be fast and non-blocking, message passing initiators will soon receive the response.

\parab{Polling and sleep.}
When an application calls non-blocking socket operations, \libipc{} polls queues of the specified FD and the emergency queue to the monitor, then returns immediately. For blocking operations (e.g., blocking recv, connect and epoll\_wait), \libipc{} first polls the queues once. If the operation is not completed, \libipc{} calls \texttt{sched\_yield} to yield to other processes on the same core. %As stated in Sec.~\ref{subsec:bottleneck}, context switch in cooperative multitasking only takes 0.4~$\mu$s. However, an application may wait a long time for an external event, making frequent wake-ups wasteful. In this regard, we count consecutive wake-ups which does not process any message, and puts the process to sleep when this reaches a threshold. 
If \libipc{} continues to yield for a certain number of rounds, it will put itself into sleep. Before sleeping, it sends a message to the monitor and all peers, so they can wake it up later through a message.

%\parab{Handling events from kernel.}
%An application often needs to poll kernel FDs (\textit{e.g.} files and semaphores) together with socket FDs.
%\libipc{} creates a per-process \textit{epoll thread} to poll kernel FDs for all application threads. When it receives a kernel event, it broadcasts the event to application threads via shared memory queues.%\texttt{Epoll\_wait} in \libipc{} will return such kernel events in addition to socket events. Note that Linux allows an event to be received by multiple threads sharing the FD.

%\parab{Exit.}
%When a process exits, the \texttt{atexit} handler of \libipc{} notifies the monitor and all peers to close connections and mark the queues as dead. However, a process may crash or get killed. In this case, monitor detects process death via \texttt{SIGHUP} of the bootstrap socket (Sec.~\ref{subsubsec:fork_fork}) and notify its peers. When a process switches to \texttt{daemon} mode or \texttt{execve} another program, it first follows the process exit procedure, then calls the system call. After that, \libipc{} is re-initialized.
